{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Pipeline\n",
    "Using what you learning about pipelining, rewrite your machine learning code from the last section to use sklearn's Pipeline. For reference, the previous main function implementation is provided in the second to last cell. Refactor this in the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Victor\n",
      "[nltk_data]     Pontello\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Victor\n",
      "[nltk_data]     Pontello\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('../data/corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    url_str_pattern = \"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
    "\n",
    "    # Identify any urls in `text`, and replace each one with the word, `\"urlplaceholder\"`.\n",
    "    # Normalize case\n",
    "    text = re.sub(url_str_pattern,'urlplaceholder',text.lower())\n",
    "    # Split `text` into tokens.\n",
    "    words = word_tokenize(text)\n",
    "    # For each token: lemmatize, and strip leading and trailing white space.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word.strip()) for word in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def display_results(y_test,y_pred):\n",
    "    labels = set(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred,normalize='true')\n",
    "    accuracy = sum(y_pred==y_test)/y_test.shape[0]\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    vect = CountVectorizer(tokenizer=tokenize)\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    # train classifier\n",
    "    X_train_counts = vect.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    X_test_counts = vect.transform(X_test)\n",
    "    X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "    # display results\n",
    "    display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    # build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf',RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "      \n",
    "        \n",
    "    # train classifier\n",
    "    pipeline.fit(X_train,y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # display results\n",
    "    display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'Dialogue', 'Action', 'Information'}\n",
      "Confusion Matrix:\n",
      " [[0.80530973 0.         0.19469027]\n",
      " [0.         0.84       0.16      ]\n",
      " [0.00215983 0.00215983 0.99568035]]\n",
      "Accuracy: 0.9534109816971714\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'Dialogue', 'Action', 'Information'}\n",
      "Confusion Matrix:\n",
      " [[0.73333333 0.         0.26666667]\n",
      " [0.03571429 0.85714286 0.10714286]\n",
      " [0.0042735  0.         0.9957265 ]]\n",
      "Accuracy: 0.9434276206322796\n"
     ]
    }
   ],
   "source": [
    "old_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines and Feature Unions\n",
    "* **FEATURE UNION**: [Feature union](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) is a class in scikit-learnâ€™s Pipeline module that allows us to perform steps in parallel and take the union of their results for the next step.\n",
    "* A **pipeline** performs a list of steps in a linear sequence, while a feature union performs a list of steps in parallel and then combines their results.\n",
    "* In more complex workflows, multiple feature unions are often used within pipelines, and multiple pipelines are used within feature unions.\n",
    "\n",
    "#### Using Feature Union\n",
    "Taking the example from the previous video, let's say you wanted to extract two different kinds of features from the same text column - tfidf values, and the length of the text. Your first approach might be to create an additional column from the `text` column called `text_length` like this. Then both `text` and `text_length` can be part of your feature matrix. But now your pipeline would break. You can't run `CountVectorizer` on NumPy arrays of strings and integers.\n",
    "\n",
    "```python\n",
    "df['txt_length'] = df['text'].apply(len)\n",
    "X = df[['text', 'txt_length']].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "\n",
    "Let's say you had a custom transformer called `TextLengthExtractor`. Now, you could leave `X_train` as just the original text column, if you could figure out how to add the text length extractor to your pipeline. If only you could fit it on the original text data, rather than the output of the previous transformer. But you need both the outputs of `TfidfTransformer` and `TextLengthExtractor` to feed into the classifier as input.\n",
    "\n",
    "```python\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('txt_length', TextLengthExtractor()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "* Feature unions are super helpful for handling these situations, where we need to run two steps in parallel on the same data and combine their results to pass into the next step.\n",
    "* Like pipelines, feature unions are built using a list of `(key, value)` pairs, where the key is the string that you want to name a step, and the value is the estimator object. Also like pipelines, feature unions combine a list of estimators to become a single estimator. However, a feature union runs its estimators in parallel, rather than in a sequence as a pipeline does. In this example, the estimators run in parallel are `nlp_pipeline` and `text_length`. Notice we use a pipeline in this feature union to make sure the count vectorizer and tfidf transformer steps are still running in sequence.\n",
    "\n",
    "```python\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('nlp_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer()\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "\n",
    "        ('txt_len', TextLengthExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "* Now, our pipeline doesn't break and uses both features! This would be equivalent to this code.\n",
    "\n",
    "```python\n",
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "txt_len = TextLengthExtractor()\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# train classifier\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "\n",
    "X_train_len = txt_len.fit_transform(X_train)\n",
    "X_train_features = hstack([X_train_tfidf, X_train_len])\n",
    "clf.fit(X_train_features, y_train)\n",
    "\n",
    "# predict on test data\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "\n",
    "X_test_len = txt_len.transform(X_test)\n",
    "X_test_features = hstack([X_test_tfidf, X_test_len])\n",
    "y_pred = clf.predict(X_test_features)\n",
    "```\n",
    "* The tfidf transformer and the text length extractor are fit to the input data, in this case the raw data, independently. They are then performed in parallel, and their outputs are combined and passed to the next estimator, in this case, the classifier.\n",
    "\n",
    "Read more about feature unions in Scikit-learn's [user guide](http://scikit-learn.org/stable/modules/pipeline.html#feature-union).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
