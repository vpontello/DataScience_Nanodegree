{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Let's incorporate grid search into your modeling process. To start, include an import statement for `GridSearchCV` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Victor\n",
      "[nltk_data]     Pontello\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Victor\n",
      "[nltk_data]     Pontello\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Victor Pontello\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('../data/corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    url_str_pattern = \"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
    "\n",
    "    # Identify any urls in `text`, and replace each one with the word, `\"urlplaceholder\"`.\n",
    "    # Normalize case\n",
    "    text = re.sub(url_str_pattern,'urlplaceholder',text.lower())\n",
    "    # Split `text` into tokens.\n",
    "    words = word_tokenize(text)\n",
    "    # For each token: lemmatize, and strip leading and trailing white space.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word.strip()) for word in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def model_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('parallel_run',FeatureUnion([\n",
    "            ('text_pipeline',Pipeline([\n",
    "                ('vect',CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf',TfidfTransformer())\n",
    "            ])),\n",
    "            ('verb_extractor',StartingVerbExtractor())\n",
    "        ])),\n",
    "        ('clf',RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def display_results(cv,y_test,y_pred):\n",
    "    labels = set(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred,normalize='true')\n",
    "    accuracy = sum(y_pred==y_test)/y_test.shape[0]\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)\n",
    "\n",
    "\n",
    "class StartingVerbExtractor(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def starting_verb(self,text):\n",
    "        sents = sent_tokenize(text)\n",
    "        for sent in sents:\n",
    "            # tokenize each sentence into words and tag part of speech\n",
    "            pos_tags = pos_tag(tokenize(sent))\n",
    "\n",
    "            # index pos_tags to get the first word and part of speech tag\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "\n",
    "            # return true if the first word is an appropriate verb or RT for retweet\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb).values\n",
    "\n",
    "        return pd.DataFrame(X_tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View parameters in pipeline\n",
    "Before modifying your build_model method to include grid search, view the parameters in your pipeline here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('parallel_run',\n",
       "   FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                   Pipeline(steps=[('vect',\n",
       "                                                    CountVectorizer(tokenizer=<function tokenize at 0x000001938A6D7E58>)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer())])),\n",
       "                                  ('verb_extractor', StartingVerbExtractor())])),\n",
       "  ('clf', RandomForestClassifier())],\n",
       " 'verbose': False,\n",
       " 'parallel_run': FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                 Pipeline(steps=[('vect',\n",
       "                                                  CountVectorizer(tokenizer=<function tokenize at 0x000001938A6D7E58>)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer())])),\n",
       "                                ('verb_extractor', StartingVerbExtractor())]),\n",
       " 'clf': RandomForestClassifier(),\n",
       " 'parallel_run__n_jobs': None,\n",
       " 'parallel_run__transformer_list': [('text_pipeline',\n",
       "   Pipeline(steps=[('vect',\n",
       "                    CountVectorizer(tokenizer=<function tokenize at 0x000001938A6D7E58>)),\n",
       "                   ('tfidf', TfidfTransformer())])),\n",
       "  ('verb_extractor', StartingVerbExtractor())],\n",
       " 'parallel_run__transformer_weights': None,\n",
       " 'parallel_run__verbose': False,\n",
       " 'parallel_run__text_pipeline': Pipeline(steps=[('vect',\n",
       "                  CountVectorizer(tokenizer=<function tokenize at 0x000001938A6D7E58>)),\n",
       "                 ('tfidf', TfidfTransformer())]),\n",
       " 'parallel_run__verb_extractor': StartingVerbExtractor(),\n",
       " 'parallel_run__text_pipeline__memory': None,\n",
       " 'parallel_run__text_pipeline__steps': [('vect',\n",
       "   CountVectorizer(tokenizer=<function tokenize at 0x000001938A6D7E58>)),\n",
       "  ('tfidf', TfidfTransformer())],\n",
       " 'parallel_run__text_pipeline__verbose': False,\n",
       " 'parallel_run__text_pipeline__vect': CountVectorizer(tokenizer=<function tokenize at 0x000001938A6D7E58>),\n",
       " 'parallel_run__text_pipeline__tfidf': TfidfTransformer(),\n",
       " 'parallel_run__text_pipeline__vect__analyzer': 'word',\n",
       " 'parallel_run__text_pipeline__vect__binary': False,\n",
       " 'parallel_run__text_pipeline__vect__decode_error': 'strict',\n",
       " 'parallel_run__text_pipeline__vect__dtype': numpy.int64,\n",
       " 'parallel_run__text_pipeline__vect__encoding': 'utf-8',\n",
       " 'parallel_run__text_pipeline__vect__input': 'content',\n",
       " 'parallel_run__text_pipeline__vect__lowercase': True,\n",
       " 'parallel_run__text_pipeline__vect__max_df': 1.0,\n",
       " 'parallel_run__text_pipeline__vect__max_features': None,\n",
       " 'parallel_run__text_pipeline__vect__min_df': 1,\n",
       " 'parallel_run__text_pipeline__vect__ngram_range': (1, 1),\n",
       " 'parallel_run__text_pipeline__vect__preprocessor': None,\n",
       " 'parallel_run__text_pipeline__vect__stop_words': None,\n",
       " 'parallel_run__text_pipeline__vect__strip_accents': None,\n",
       " 'parallel_run__text_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'parallel_run__text_pipeline__vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'parallel_run__text_pipeline__vect__vocabulary': None,\n",
       " 'parallel_run__text_pipeline__tfidf__norm': 'l2',\n",
       " 'parallel_run__text_pipeline__tfidf__smooth_idf': True,\n",
       " 'parallel_run__text_pipeline__tfidf__sublinear_tf': False,\n",
       " 'parallel_run__text_pipeline__tfidf__use_idf': True,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__ccp_alpha': 0.0,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__max_samples': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 100,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = model_pipeline()\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify your `build_model` function to return a GridSearchCV object.\n",
    "Try to grid search some parameters in your data transformation steps as well as those for your classifier! Browse the parameters you can search above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    pipeline = model_pipeline()\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "        'clf__n_estimators':[10,50,100,200,400],\n",
    "        'clf__bootstrap':[True,False],\n",
    "        'parallel_run__text_pipeline__tfidf__smooth_idf': [True,False]\n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run program to test\n",
    "Running grid search can take a while, especially if you are searching over a lot of parameters! If you want to reduce it to a few minutes, try commenting out some of your parameters to grid search over just 1 or 2 parameters with a small number of values each. Once you know that works, feel free to add more parameters and see how well your final model can perform! You can try this out in the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'Information', 'Action', 'Dialogue'}\n",
      "Confusion Matrix:\n",
      " [[0.78947368 0.         0.21052632]\n",
      " [0.         0.87878788 0.12121212]\n",
      " [0.00660793 0.         0.99339207]]\n",
      "Accuracy: 0.9484193011647255\n",
      "\n",
      "Best Parameters: {'clf__bootstrap': False, 'clf__n_estimators': 50, 'parallel_run__text_pipeline__tfidf__smooth_idf': False}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    display_results(model, y_test, y_pred)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
