{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering\n",
    "\n",
    "**Data engineers** gather data from various sources, process, combine and manipulate the data so it can be easily accessed and used. This whole process from gathering data to getting it ready to use can be automated by software programs, also called **data pipelines**.\n",
    "\n",
    "Data engineers are responsible for gather data from various sources, process, and combine and the data so it can be easily accessed and used. This process is often automated by data pipelines.\n",
    "\n",
    "As a data scientist, it's important to have some level of skill in data engineering despite the size of the company you work in.\n",
    "\n",
    "In larger companies, although there are dedicated data engineers, data scientists need to communicate with data engineers about the data they need for the data science project. In smaller companies, data scientists may need to take some of the responsibility of a data engineer.\n",
    "\n",
    "## Data Pipelines: ETL vs ELT\n",
    "Data pipeline is a generic term for moving data from one place to another. For example, it could be moving data from one server to another server.\n",
    "\n",
    "### ETL\n",
    "An ETL pipeline is a specific kind of data pipeline and very common. ETL stands for Extract, Transform, Load. Imagine that you have a database containing web log data. Each entry contains the IP address of a user, a timestamp, and the link that the user clicked.\n",
    "\n",
    "What if your company wanted to run an analysis of links clicked by city and by day? You would need another data set that maps IP address to a city, and you would also need to extract the day from the timestamp. With an ETL pipeline, you could run code once per day that would extract the previous day's log data, map IP address to city, aggregate link clicks by city, and then load these results into a new database. That way, a data analyst or scientist would have access to a table of log data by city and day. That is more convenient than always having to run the same complex data transformations on the raw web log data.\n",
    "\n",
    "Before cloud computing, businesses stored their data on large, expensive, private servers. Running queries on large data sets, like raw web log data, could be expensive both economically and in terms of time. But data analysts might need to query a database multiple times even in the same day; hence, pre-aggregating the data with an ETL pipeline makes sense.\n",
    "\n",
    "### ELT\n",
    "ELT (Extract, Load, Transform) pipelines have gained traction since the advent of cloud computing. Cloud computing has lowered the cost of storing data and running queries on large, raw data sets. Many of these cloud services, like Amazon Redshift, Google BigQuery, or IBM Db2 can be queried using SQL or a SQL-like language. With these tools, the data gets extracted, then loaded directly, and finally transformed at the end of the pipeline.\n",
    "\n",
    "However, ETL pipelines are still used even with these cloud tools. Oftentimes, it still makes sense to run ETL pipelines and store data in a more readable or intuitive format. This can help data analysts and scientists work more efficiently as well as help an organization become more data driven.\n",
    "\n",
    "## World Bank Data\n",
    "This lesson uses data from the World Bank. The data comes from two sources:\n",
    "\n",
    "1. [World Bank Indicator Data](https://data.worldbank.org/indicator) - This data contains socio-economic indicators for countries around the world. A few example indicators include population, arable land, and central government debt.\n",
    "2. [World Bank Project Data](https://datacatalog.worldbank.org/dataset/world-bank-projects-operations) - This data set contains information about World Bank project lending since 1947.\n",
    "\n",
    "Both of these data sets are available in different formats including as a csv file, json, or xml. You can download the csv directly or you can use the World Bank APIs to extract data from the World Bank's servers. You'll be doing both in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the data file types you'll work with\n",
    "### CSV files\n",
    "CSV stands for comma-separated values. These types of files separate values with a comma, and each entry is on a separate line. Oftentimes, the first entry will contain variable names. Here is an example of what CSV data looks like. This is an abbreviated version of the first three lines in the World Bank projects data csv file.\n",
    "```\n",
    "id,regionname,countryname,prodline,lendinginstr\n",
    "P162228,Other,World;World,RE,Investment Project Financing\n",
    "P163962,Africa,Democratic Republic of the Congo;Democratic Republic of the Congo,PE,Investment Project Financing\n",
    "```\n",
    "### JSON\n",
    "JSON is a file format with key/value pairs. It looks like a Python dictionary. The exact same CSV file represented in JSON could look like this:\n",
    "```json\n",
    "[{\"id\":\"P162228\",\"regionname\":\"Other\",\"countryname\":\"World;World\",\"prodline\":\"RE\",\"lendinginstr\":\"Investment Project Financing\"},{\"id\":\"P163962\",\"regionname\":\"Africa\",\"countryname\":\"Democratic Republic of the Congo;Democratic Republic of the Congo\",\"prodline\":\"PE\",\"lendinginstr\":\"Investment Project Financing\"},{\"id\":\"P167672\",\"regionname\":\"South Asia\",\"countryname\":\"People\\'s Republic of Bangladesh;People\\'s Republic of Bangladesh\",\"prodline\":\"PE\",\"lendinginstr\":\"Investment Project Financing\"}]\n",
    "```\n",
    "Each line in the data is inside of a squiggly bracket {}. The variable names are the keys, and the variable values are the values.\n",
    "\n",
    "There are other ways to organize JSON data, but the general rule is that JSON is organized into key/value pairs. For example, here is a different way to represent the same data using JSON:\n",
    "```json\n",
    "{\"id\":{\"0\":\"P162228\",\"1\":\"P163962\",\"2\":\"P167672\"},\"regionname\":{\"0\":\"Other\",\"1\":\"Africa\",\"2\":\"South Asia\"},\"countryname\":{\"0\":\"World;World\",\"1\":\"Democratic Republic of the Congo;Democratic Republic of the Congo\",\"2\":\"People\\'s Republic of Bangladesh;People\\'s Republic of Bangladesh\"},\"prodline\":{\"0\":\"RE\",\"1\":\"PE\",\"2\":\"PE\"},\"lendinginstr\":{\"0\":\"Investment Project Financing\",\"1\":\"Investment Project Financing\",\"2\":\"Investment Project Financing\"}}\n",
    "```\n",
    "### XML\n",
    "Another data format is called XML (Extensible Markup Language). XML is very similar to HTML at least in terms of formatting. The main difference between the two is that HTML has pre-defined tags that are standardized. In XML, tags can be tailored to the data set. Here is what this same data would look like as XML.\n",
    "\n",
    "```xml\n",
    "<ENTRY>\n",
    "  <ID>P162228</ID>\n",
    "  <REGIONNAME>Other</REGIONNAME>\n",
    "  <COUNTRYNAME>World;World</COUNTRYNAME>\n",
    "  <PRODLINE>RE</PRODLINE>\n",
    "  <LENDINGINSTR>Investment Project Financing</LENDINGINSTR>\n",
    "</ENTRY>\n",
    "<ENTRY>\n",
    "  <ID>P163962</ID>\n",
    "  <REGIONNAME>Africa</REGIONNAME>\n",
    "  <COUNTRYNAME>Democratic Republic of the Congo;Democratic Republic of the Congo</COUNTRYNAME>\n",
    "  <PRODLINE>PE</PRODLINE>\n",
    "  <LENDINGINSTR>Investment Project Financing</LENDINGINSTR>\n",
    "</ENTRY>\n",
    "<ENTRY>\n",
    "  <ID>P167672</ID>\n",
    "  <REGIONNAME>South Asia</REGIONNAME>\n",
    "  <COUNTRYNAME>People's Republic of Bangladesh;People's Republic of Bangladesh</COUNTRYNAME>\n",
    "  <PRODLINE>PE</PRODLINE>\n",
    "  <LENDINGINSTR>Investment Project Financing</LENDINGINSTR>\n",
    "</ENTRY>\n",
    "```\n",
    "\n",
    "ML is falling out of favor especially because JSON tends to be easier to navigate; however, you still might come across XML data. The World Bank API, for example, can return either XML data or JSON data. From a data perspective, the process for handling HTML and XML data is essentially the same.\n",
    "\n",
    "### SQL databases\n",
    "SQL databases store data in tables using [primary and foreign keys](https://docs.microsoft.com/en-us/sql/relational-databases/tables/primary-and-foreign-key-constraints?view=sql-server-2017). In a SQL database, the same data would look like this:\n",
    "\n",
    "|id\t| regionname\t|countryname|\tprodline|\tlendinginstr |\n",
    "|---|-------------|-----------|---------|--------------|\n",
    "|P162228\t|Other\t|World;World\t|RE\t|Investment Project Financing|\n",
    "|P163962\t|Africa\t|Democratic Republic of the Congo;Democratic Republic of the Congo\t|PE\t|Investment Project Financing|\n",
    "|P167672\t|South Asia|\tPeople's Republic of Bangladesh;People's Republic of Bangladesh\t|PE|\tInvestment Project Financing|\n",
    "\n",
    "### Text Files\n",
    "This course won't go into much detail about text data. There are other Udacity courses, namely on natural language processing, that go into the details of processing text for machine learning.\n",
    "\n",
    "Text data present their own issues. Whereas CSV, JSON, XML, and SQL data are organized with a clear structure, text is more ambiguous. For example, the World Bank project data country names are written like this\n",
    "```\n",
    "Democratic Republic of the Congo;Democratic Republic of the Congo\n",
    "```\n",
    "In the World Bank Indicator data sets, the Democratic Republic of the Congo is represented by the abbreviation \"Congo, Dem. Rep.\" You'll have to clean these country names to join the data sets together.\n",
    "\n",
    "### Extracting Data from the Web\n",
    "In this lesson, you'll see how to extract data from the web using an APIs (Application Programming Interface). APIs generally provide data in either JSON or XML format.\n",
    "\n",
    "Companies and organizations provide APIs so that programmers can access data in an official, safe way. APIs allow you to download, and sometimes even upload or modify, data from a web server without giving you direct access.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "Transforming data means getting data ready for a machine learning algorithm or other data science projects. Transforming data can involve a wide range of processes such as combining data from different sources, cleaning the data, engineering new features. Hence, you transform the original datasets to create a new dataset that is ready for use.\n",
    "\n",
    "### Combining Data\n",
    "\n",
    "Oftentimes you need to combine data from different datasets. The process of combining data can be complicated and very different from case to case.\n",
    "\n",
    "Datasets can be from different sources and have different content so you need to check the data closely before combining them. Or they may be in different formats meaning you will need to transform data from one format to another first.\n",
    "\n",
    "In the next part of the lesson, you will have an exercise for combining data from different sources using the Python Pandas package. If you are unfamiliar with the Pandas, you can review the learning resources below to prepare yourself before heading to the exercise.\n",
    "\n",
    "### Cleaning Data\n",
    "\n",
    "Dirty data refers to data that contains errors. Data error can come from many sources including:\n",
    "\n",
    "* data entry mistakes\n",
    "* duplicate data\n",
    "* incomplete records\n",
    "* inconsistencies between dataset\n",
    "\n",
    "It's very important to audit data after obtaining it, otherwise, the data science projects will not perform well or even worse, give you wrong results.\n",
    "\n",
    "In the next section, you will have an exercise on data cleaning. Your job is to clean the data so the country names across different datasets are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "In the video, I say that a machine learning algorithm won't work with missing values. This is essentially correct; however, there are a couple of situations where this isn't quite true. For example, if you had a categorical variable, you could keep the NULL value as one of the options.\n",
    "\n",
    "Like if theme_2 could have a value of agriculture, banking, or NULL, you might encode this variable as 0, 1, 2 where the value 2 stands in for the NULL value. You could do something similar for one-hot encoding where the theme_2 variable becomes 3 true/false features: theme_2_agriculture, theme_2_banking, theme_2_NULL. You could have to make sure that this improves your model performance.\n",
    "\n",
    "There are also implementations of some machine learning algorithms, such as [gradient boosting](https://xgboost.readthedocs.io/en/latest/) decision trees that can [handle missing values](https://github.com/dmlc/xgboost/issues/21).\n",
    "\n",
    "### Missing Data - Delete\n",
    "There are two ways to handle missing values:\n",
    "\n",
    "* Delete data\n",
    "* Fill missing values (also called imputation)\n",
    "\n",
    "You can delete a feature with a large percentage of missing values because it's unlikely to contribute to your machine learning model unless somehow you can find the missing values from another source. If you think deleting a row consisting of a lot of missing values won't affect your result, you can also delete it.\n",
    "\n",
    "However, deleting missing values is not the only option. In the next part, we will talk about another option - fill in missing values.\n",
    "\n",
    "### Missing Data - Inpute\n",
    "The process of filling in missing values is called `imputation`. Some of the imputation techniques are mean substitution, forward fill, and backward fill.\n",
    "\n",
    "##### Mean Substitution\n",
    "The mean substitution method fills in the missing values using the `column mean`. You can even group the data first then filling the data by the means in different groups. Alternatively, you can fill in the missing values by `median` or `mode`.\n",
    "\n",
    "##### Forward Fill or Backward Fill\n",
    "Forward fill or backward fill works for `ordered` or `times series` data. In both methods, you can use neighboring cells to fill in the missing value.\n",
    "\n",
    "To use these methods, you should always make sure your data is `sorted` by a timestamp or in a meaningful way. With the forward fill, values are pushed forward `down` to replace any missing values. With backward fill, values move `up` to fill missing values.\n",
    "\n",
    "## Duplicate Data\n",
    "Data duplication is obvious when the same row shows up more than once and you can simply remove the duplicate data using the Pandas drop duplicates method. However, duplicate data can sometimes be trickier to find which requires you to comb through the data to recognize and eliminate duplications.\n",
    "\n",
    "## Dummy Variables\n",
    "\n",
    "### When to Remove a Feature\n",
    "As mentioned in the video, if you have five categories, you only really need four features. For example, if the categories are \"agriculture\", \"banking\", \"retail\", \"roads\", and \"government\", then you only need four of those five categories for dummy variables. This topic is somewhat outside the scope of a data engineer.\n",
    "\n",
    "In some cases, you don't necessarily need to remove one of the features. It will depend on your application. In regression models, which use linear combinations of features, removing a dummy variable is important. For a decision tree, removing one of the variables is not needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "### Outliers - How to Find Them\n",
    "**Outliers** are data points that are far away from the rest of the data. Outliers can be caused by errors such as data entry errors or processing mistakes. However, outliers can also be legitimate data.\n",
    "\n",
    "There are some methods to detect outliers:\n",
    "\n",
    "**Data visualization**\n",
    "When working with one or two-dimensional data, you can visualize data to detect outliers. The data points far away from the rest of the data on the plot can potentially be outliers.\n",
    "\n",
    "**Statistical methods**\n",
    "Statistical properties of data like means, standard deviations, quantiles can be used to identify outliers, for example, the z-score from the norma distribution and the Tukey method.\n",
    "\n",
    "**Machine learning methods**\n",
    "When it comes to high-dimensional data, you can use machine learning techniques such as PCA to reduce the data dimensions. Then you can use the methods discussed before to detect outliers.\n",
    "\n",
    "Another way is to cluster the data then calculate the distance from each data point to the cluster centroid. A large distance may indicate an outlier.\n",
    "\n",
    "Next, you will get some practice on identifying outliers.\n",
    "\n",
    "#### Outlier Detection Resources [Optional]\n",
    "Here are a couple of links to outlier detection processes and algorithms. Since this is an ETL course rather than a statistics course, you don't need to read these in order to complete the lesson.\n",
    "\n",
    "* [scikit-learn novelty and outlier detection](http://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "* [statistical and machine learning methods for outlier detection](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561)\n",
    "\n",
    "#### Tukey Rule\n",
    "* Find the first quartile (ie .25 quantile)\n",
    "* Find the third quartile (ie .75 quantile)\n",
    "* Calculate the inter-quartile range (Q3 - Q1)\n",
    "* Any value that is greater than Q3 + 1.5 * IQR is an outlier\n",
    "* Any value that is less than Qe - 1.5 * IQR is an outlier\n",
    "\n",
    "### Outliers - What to do\n",
    "Now you've identified the outliers then what next? The answer varies depending on how the outlier affects your machine learning model. If removing the outlier improves your machine learning model, maybe it's best to delete the outliers. But if the outlier has a special meaning so it has to be taken into account in your model, then it's best to keep it or find other solutions. In another case, if removing the outlier has little or no effect on your results, you can leave the outliers as it.\n",
    "\n",
    "## Scaling Data\n",
    "Numerical data comes in all different distribution patterns and ranges. This can be an issue for machine learning algorithms that calculate Euclidean distance between points, such as PCA, linear regression with gradient descent. This issue can be solved by performing data **normalization** or **feature scaling**. Two common ways to scale features are **rescaling** and **standardization**.\n",
    "\n",
    "### Rescaling / Normalization\n",
    "With rescaling, the distribution of the data remains the same but the range changes to 0-1. You scale down the data range so the minimum value is zero and the maximum value is one.\n",
    "\n",
    "To normalize data, you take a feature, like gdp, and use the following formula\n",
    "\n",
    "$x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "where \n",
    "* x is a value of gdp\n",
    "* x_max is the maximum gdp in the data\n",
    "* x_min is the minimum GDP in the data\n",
    "\n",
    "\n",
    "### Standardization\n",
    "With standardization, the general shape of the distribution remains the same but the mean and standard deviation are standardized. You transform the data so that it has a mean of zero and a standard deviation of one.\n",
    "\n",
    "## Feature Engineering\n",
    "Feature engineering refers to the process to create new features and it is a very broad topic. Data engineers wouldn't necessarily decide what feature is a good one to engineer but they might be asked to write code that transforms data into a new feature.\n",
    "\n",
    "In the video above, we looked at taking the polynomial of data as a feature engineering example. Using the two features `x` and `y` , you can create many new features like `x^2`, `x^3`, `xy`, `y^2`, `x^2y`, and so on. Creating new features is especially useful if your model is underfitting in which existing features can't capture the trend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "**Load** refers to store the data in a database. There are many options for data storage.\n",
    "\n",
    "* Relational database like SQL works well with structured data\n",
    "* CSV files work well with data that fits in a Pandas DataFrame\n",
    "\n",
    "Part of the data engineers' job is to know how to work with different data storage options.\n",
    "\n",
    "#### Links to Other Data Storage Systems\n",
    "* ranking of database engines\n",
    "* Redis\n",
    "* Cassandra\n",
    "* Hbase\n",
    "* MongoDB\n",
    "\n",
    "## Lesson Summary\n",
    "\n",
    "In this lesson, you've learned to create ETL pipelines: you pull the data from different sources, transform the data using various techniques and load the transformed data in a data storage.\n",
    "\n",
    "#### I. Extract data from different sources\n",
    "* csv files\n",
    "* json files\n",
    "* APIs\n",
    "\n",
    "#### II. Transform data\n",
    "* combining data from different sources\n",
    "* data cleaning\n",
    "* data types\n",
    "* parsing dates\n",
    "* file encodings\n",
    "* missing data\n",
    "* duplicate data\n",
    "* dummy variables\n",
    "* remove outliers\n",
    "* scaling features\n",
    "* engineering features\n",
    "\n",
    "#### III. Load data\n",
    "* send the transformed data to a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipelines\n",
    "\n",
    "In this lesson, you'll be introduced to some of the steps involved in a NLP pipeline:\n",
    "\n",
    "1. Text Processing\n",
    "    * Cleaning\n",
    "    * Normalization\n",
    "    * Tokenization\n",
    "    * Stop Word Removal\n",
    "    * Part of Speech Tagging\n",
    "    * Named Entity Recognition\n",
    "    * Stemming and Lemmatization\n",
    "2. Feature Extraction\n",
    "    * Bag of Words\n",
    "    * TF-IDF\n",
    "    * Word Embeddings\n",
    "3. Modeling\n",
    "\n",
    "## How NLP Pipelines Work\n",
    "The 3 stages of an NLP pipeline are: Text Processing > Feature Extraction > Modeling.\n",
    "\n",
    "* **Text Processing**: Take raw input text, clean it, normalize it, and convert it into a form that is suitable for feature extraction.\n",
    "* **Feature Extraction**: Extract and produce feature representations that are appropriate for the type of NLP task you are trying to accomplish and the type of model you are planning to use.\n",
    "* **Modeling**: Design a statistical or machine learning model, fit its parameters to training data, use an optimization procedure, and then use it to make predictions about unseen data.\n",
    "\n",
    "This process isn't always linear and may require additional steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "\n",
    "### Stage 1: Text Processing\n",
    "The first chunk of this lesson will explore the steps involved in text processing, the first stage of the NLP pipeline.\n",
    "\n",
    "You'll prepare text data from different sources with the following text processing steps:\n",
    "\n",
    "1. **Cleaning** to remove irrelevant items, such as HTML tags\n",
    "2. **Normalizing** by converting to all lowercase and removing punctuation\n",
    "3. Splitting text into words or **tokens**\n",
    "4. Removing words that are too common, also known as **stop words**\n",
    "5. Identifying different **parts of speech** and **named entities**\n",
    "6. Converting words into their dictionary forms, using **stemming and lemmatization**\n",
    "\n",
    "After performing these steps, your text will capture the essence of what was being conveyed in a form that is easier to work with.\n",
    "\n",
    "#### Why Do We Need to Process Text?\n",
    "* **Extracting plain text**: Textual data can come from a wide variety of sources: the web, PDFs, word documents, speech recognition systems, book scans, etc. Your goal is to extract plain text that is free of any source specific markup or constructs that are not relevant to your task.\n",
    "* **Reducing complexity**: Some features of our language like capitalization, punctuation, and common words such as a, of, and the, often help provide structure, but don't add much meaning. Sometimes it's best to remove them if that helps reduce the complexity of the procedures you want to apply later.\n",
    "\n",
    "### Cleaning\n",
    "Let's walk through an example of cleaning text data from a popular source - the web. You'll be introduced to helpful tools in working with this data, including the **requests** library, **regular expressions**, and **Beautiful Soup**.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# Fetch a web page\n",
    "r= requests.get(\"https://www.udacity.com/courses/all\")\n",
    "print(r.text)\n",
    "```\n",
    "> Outputs Entire HTML Source\n",
    "\n",
    "This seems inefficient so let's use the `BeautifulSoup` library to help.\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Remove HTML tags using Beautiful Soup Library\n",
    "soup = BeautifulSoup(r.text, \"html5lib\") # pass in raw web page and create a soup object\n",
    "print(soup.get_text()) # extract plain text from all the tags\n",
    "```\n",
    "> Outputs No tags but there is still JavaScript and tons of white space.\n",
    "\n",
    "Go to the webpage and take a Inspect the element you are interested in. Let's first look at the `course-summary-card`\n",
    "```python\n",
    "# Find all course summaries\n",
    "summaries = soup.find_all(\"div\", class_=\"course-summary-card\")\n",
    "summaries[0]\n",
    "```\n",
    "> Outputs A list of all the `course-summary-card` values and will print out the 1st item in that list.\n",
    "\n",
    "Looking at the summary we find that the title is nested in an `<a>` tag inside an `<h3>` tag.\n",
    "```python\n",
    "# Extract title\n",
    "summaries[0].select_one(\"h3 a\").get_text().strip()\n",
    "```\n",
    "> Output:  Intro to Programming Nanodegree\n",
    "\n",
    "Going back we can also see the summary is in a `<div data-course-short-summary=\"\">` tag.\n",
    "```python\n",
    "# Extract description\n",
    "summaries[0].select_one(\"div[data-course-short-summary]\").get_text().strip()\n",
    "```\n",
    "> Output: \n",
    "> *Udacity's Intro to Programming is your first step towards careers in Web and App Development, Machine Learning, Data Science, AI, and more! This program is perfect for beginners.*\n",
    "\n",
    "```python\n",
    "# Find all course summaries, extract title and descriptiion\n",
    "courses = []\n",
    "summaries = soup.find_all(\"div\", class_=\"course-summary-card\")\n",
    "for summary in summaries: \n",
    "  title = summary.select_one(\"h3 a\").get_text().strip() \n",
    "  description = summary.select_one(\"div[data-course-short-summary]\").get_text().strip()\n",
    "  print(\"***\", title, \"***\")\n",
    "  print(description)\n",
    "\n",
    "print(len(courses), \"course summaries found. Sample:\") \n",
    "print(courses[0][0])\n",
    "print(courses[0][1])\n",
    "```\n",
    "\n",
    "> Output: 193 course summaries found. Sample:\n",
    "Intro to Programming Nanodegree\n",
    "Udacity's Intro to Programming is your first step towards careers in Web and App Development, Machine Learning, Data Science, AI, and more! This program is perfect for beginners.\n",
    "\n",
    "#### Documentation for Python Libraries:\n",
    "* [Requests](http://docs.python-requests.org/en/master/user/quickstart/#make-a-request)\n",
    "* [Regular Expressions](https://docs.python.org/3/library/re.html)\n",
    "* [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Plain text is still human language with all its variations and bells and whistles so in normalization, we will try to reduce some of that complexity.\n",
    "\n",
    "#### Capitalization Removal\n",
    "In the English language, the starting letter of the first word in any sentence is usually capitalized. All caps are sometimes used for emphasis and for stylistic reasons. While this is convenient for a human reader from the standpoint of a machine learning algorithm, it does not make sense to differentiate between variations that mean the same thing:\n",
    "\n",
    "* Car\n",
    "* car\n",
    "* CAR\n",
    "\n",
    "Therefore, we usually convert every letter in our text to a common case, usually lowercase, so that each word is represented by a unique token.\n",
    "\n",
    "Here's some sample text from a movie review:\n",
    "> The first time you see The Second Renaissance it may look boring. Look at it at least twice and definetly watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\n",
    "\n",
    "If we have the review stored in a variable called text, converting it to lowercase is a simple call to the lower method in Python.\n",
    "```python\n",
    "# Conver to lowercase\n",
    "text = text.lower()\n",
    "print(text)\n",
    "```\n",
    "> **Output**\n",
    "the first time you see the second renaissance it may look boring. look at it at least twice and definetly watch part 2. it will change your view of the matrix. are the human people the ones who started the war ? is ai a bad thing ?\n",
    "\n",
    "Note all the letters that were changed.\n",
    "\n",
    "#### Punctation Removal\n",
    "Other languages may or may not have a case equivalent but similar principles may apply depending on your NLP task, you may want to remove special characters like periods, question marks, and exclamation points from the text and only keep letters of the alphabet and maybe numbers.\n",
    "\n",
    "This is useful when looking at text documents as a whole in applications like document classification and clustering where the low level details doesn't affect the application.\n",
    "\n",
    "To do this we can use a regular expression that matches everything that is not a lowercase A to Z, uppercase A is Z, or digits zero to nine, and replaces them with a space.\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Remove punctuation characters\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text) # Anything that isn't A through Z or 0 through 9 will be replaced by a space\n",
    "print(text)\n",
    "```\n",
    "> **Output**\n",
    "the you he first time you see the second renaissance it may look boring look at it at least twice and definetly watch part 2 it will change your view of the matrix are the human people the ones who started the war is ai a bad thing\n",
    "\n",
    "This approach avoids having to specify all punctuation characters, but you can use other regular expressions as well.\n",
    "\n",
    "Lowercase conversion and punctuation removal are the two most common text normalization steps. If and when you apply these steps depends on your end goal and how you design your pipeline.\n",
    "\n",
    "> It is better to replace the punctuation with a space than tho remove them. Replacing with a space makes sure that words don't get concatenated together, in case the original text did not have a space before or after the punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Token is a fancy term for a symbol that holds some meaning and is not typically split up any further.\n",
    "\n",
    "In natural language processing, our tokens are usually individual words. This means that the process of tokenization is simply splitting a sentence into a sequence of words. The simplest way to do this is using the split method which returns a list of words.\n",
    "\n",
    "> **Input**\n",
    "the you he first time you see the second renaissance it may look boring look at it at least twice and definetly watch part 2 it will change your view of the matrix are the human people the ones who started the war is ai a bad thing\n",
    "\n",
    "```python\n",
    "# Split text into tokens (words)\n",
    "words = text.split()\n",
    "print(words)\n",
    "```\n",
    "> **Output**\n",
    "['the', 'you', 'he', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definetly', 'watch', 'part', '2', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human', 'people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing']\n",
    "\n",
    "Notice that it splits on whitespace characters (spaces, tabs, new lines, etc.) and will automatically ignoring two or more whitespace characters in a sequence, so it doesn't return blank strings. This can be further adjusted using optional parameters.\n",
    "\n",
    "### Natural Language Toolkit (NLTK)\n",
    "So far, we've only been using Python's built-in functionality, but some of these operations are much easier to perform using a library like Natural Language Toolkit (NLTK).\n",
    "\n",
    "The most common approach for splitting up text in NLTK is to use the word tokenized function from nltk.tokenize.\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Split text into words using NLTK\n",
    "words = word_tokenize(text)\n",
    "print(words)\n",
    "```\n",
    "This performs the same task as split but has a few more features than the split method.\n",
    "\n",
    "For example if we gave it\n",
    "> Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers.\n",
    "\n",
    "it would return the following\n",
    "\n",
    "> ['Dr.', 'Smith', 'graduated', 'from', 'the', 'University', 'of', 'Washington', '.', 'He', 'later', 'started', 'an', 'analytics', 'firm', 'called', 'Lux', ',', 'which', 'catered', 'to', 'enterprise', 'customers', '.']\n",
    "\n",
    "You'll notice that the punctuations are treated differently based on their position. For example, 'Dr.' has been tokenized as one word rather than being tokenized into two seperate entities 'Dr' and '.'. NLTK is using some rules or patterns to decide what to do with each punctuation.\n",
    "\n",
    "#### NLTK's Sentence Tokenization\n",
    "There are instances you may need to split a longer docuument into sentence, this is something that might be done for translations. You can achieve this with NLTK using sent tokenize.\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences) \n",
    "```\n",
    "> **Output**\n",
    "['Dr.Smith graduated from the University of Washington.', 'He later started an analytics firm called Lux, which catered to enterprise customers.']\n",
    "\n",
    "Now one could tokenize based on words if needed.\n",
    "\n",
    "NLTK provide several other tokenizers and here are some of them:\n",
    "\n",
    "* regular expression based tokenizer that can remove punctuation and perform tokenization in a single step\n",
    "* tweet tokenizer that is aware of twitter handles, hash tags, and emoticons\n",
    "Reference:\n",
    "\n",
    "* nltk.tokenize package: http://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal\n",
    "**Stop Words** are words that don't add a lot meaning to a sentence or phrase (i.e, is, the, in, at, etc.) and are often very common words.\n",
    "\n",
    "We want to remove them to simplify procedures down the pipeline.\n",
    "\n",
    "For example you may have the statement:\n",
    "> Dogs are the best\n",
    "\n",
    "Even with removing \"are\" and \"the\", the positive sentiment about dogs is still conveyed.\n",
    "\n",
    "A common package that has a pre-set list of stop words is NLTK.\n",
    "\n",
    "```python\n",
    "# List stop words from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))\n",
    "```\n",
    "\n",
    "The NLTK can be used on a list of words.\n",
    "```python\n",
    "words = ['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definetly', 'watch', 'part', '2', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing']\n",
    "\n",
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speach Tagging\n",
    "**Note**: Part-of-speech tagging using a predefined grammar like this is a simple, but limited, solution. It can be very tedious and error-prone for a large corpus of text, since you have to account for all possible sentence structures and tags!\n",
    "\n",
    "There are other more advanced forms of POS tagging that can learn sentence structures and tags from given data, including Hidden Markov Models (HMMs) and Recurrent Neural Networks (RNNs).\n",
    "\n",
    "NLTK has the ability to label the parts of speach of the words given.\n",
    "\n",
    "```python\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Tag parts of speach (PoS)\n",
    "sentence = word_tokenize(\"I always lie down to tell a lie.\")\n",
    "pos_tag(sentence)\n",
    "```\n",
    "output\n",
    "```python\n",
    "[('I', 'PRP'),\n",
    " ('always', 'RB'),\n",
    " ('lie', 'VBP'),\n",
    " ('down', 'RP'),\n",
    " ('to', 'TO'),\n",
    " ('tell', 'VB'),\n",
    " ('a', 'DT'),\n",
    " ('lie', 'NN'),\n",
    " ('.', '.')]\n",
    "```\n",
    "Custom grammar to parse an ambiguous sentence and will return the possible ways the sentence could be read.\n",
    "```python\n",
    "# Define a cusom grammar\n",
    "my_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(my_grammar)\n",
    "\n",
    "# Parse a sentence\n",
    "sentence = word_tokenize(\"I shot an elephant in my pajamas\")\n",
    "for tree in parser.parse(sentence):\n",
    "  print(tree)\n",
    "```\n",
    "This can be even further visualized with the draw function on the tree\n",
    "```python\n",
    "# Visualize parse trees\n",
    "for tree in parser.parse(sentence):\n",
    "  tree.draw()\n",
    "```\n",
    "To learn more about NLTK PoS\n",
    "\n",
    "* NLTK Documentation on pos_tag in this link to [Chapter 5. Categorizing and Tagging Words](http://www.nltk.org/book/ch05.html)\n",
    "* Stack Overflow thread on the tokens for pos_tag in this link to [What are all possible pos tags of NLTK](https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "Named Entity are nouns or noun phrases that refer to specific object, person, or place.\n",
    "\n",
    "To label these we can use the `ne_chunk` function in NLTK.\n",
    "```python\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Recognize named entities in a tagged sentence\n",
    "ne_chunk(pos_tag(word_tokenize(\"Antonio joined Udacity Inc. in California.\")))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "#### Stemming\n",
    "Stemming is the process of reducing a word to its stem or root form. For example, branching, branched, and branches all stem from the word branch.\n",
    "\n",
    "This is a very quick and rough process so sometime the result isn't a complete word. For example, caching, cached, caches would result in a stem \"cach\", but that isn't a word. But as long as all related words to cache results in the same stem still captures the common idea in the resultant stem.\n",
    "\n",
    "There are a few options from NLTK but in this example we will look at Porter.\n",
    "```python\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)\n",
    "```\n",
    "\n",
    "#### Lemmatization\n",
    "**Lemmatization** is the process to map the words back to its root using a dictionary. For example, is, was, and were would all be lemmatized to \"be\".\n",
    "\n",
    "The default NLTK lemmatizer is wordnet.\n",
    "\n",
    "```python\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed)\n",
    "```\n",
    "Lemmatizers need to know the part of speech and will default to nouns but we can add parameters to change which part of speech it will use.\n",
    "```python\n",
    "# Lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "print(lemmed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing Summary\n",
    "Let's go over a summary of all the text processing we just covered.\n",
    "\n",
    "|Text  Processing | Example |\n",
    "|--|--|\n",
    "|Given\t|Jenna went back to University|\n",
    "|Normalized\t|jenna went back to university|\n",
    "|Tokenized\t|<\"jenna\", \"went\", \"back\", \"to\", \"University\">|\n",
    "|Stop Word Removal\t|<\"jenna\", \"went\", \"university\">|\n",
    "|Stem & Lemmatized\t|<\"jenna\", \"go\", \"univers\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Each letter is represented using encodings like ASCII or Unicode so that each letter is represented by a number which is then stored or transmitted using binary (0s or 1s).\n",
    "\n",
    "Words, rather than letters themselves, hold meaning. But computers don't have a standard representation for words. Practically they are a sequence of binary, ASCII, or Unicode but the meaning and relationship between words is not easily captured with these methods.\n",
    "\n",
    "In comparison, an image's pixel value contains the relative intensity of light. For a color image we keep a value for each of the primary colors (red, green, and blue) which carry relavant information. This means that pixels with similar values are also visually similar. So pixel values can be used in a numerical model for images.\n",
    "\n",
    "#### How can we do the same thing for image modelsing with language?\n",
    "\n",
    "Depends on the model and goal of the model.\n",
    "\n",
    "For example, for a graph based model to extract insights you might create a web of nodes.\n",
    "\n",
    "But if you want a statistical model, you will need numerical representation.\n",
    "\n",
    "* If you are working at the document level (for spam detection or sentiment of the document) one would use bag-of-words or doc2vec.\n",
    "* If you are working at the individual words and phrases (for text generation or machine translation) one would use word2vec or glove.\n",
    "Practice will help over time to determine which is the best method for your use case.\n",
    "\n",
    "WordNet visualization tool: http://mateogianolio.com/wordnet-visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "Each document is turned into an unordered collection of words. For a plagiarism check for students in a class each submission or report could be considered a document. But if you are looking at sentiment in a tweet, each tweet would be considered a document.\n",
    "\n",
    "The first step is text processing and below is a table of the Given and result after text processing.\n",
    "\n",
    "|Given | Text Processed |\n",
    "|--|--|\n",
    "|Little House on the Prairie | {\"littl\", \"hous\", \"prairi\"}|\n",
    "|Mary had a Little Lamb\t| {\"mari\", \"littl\", \"lamb\"}|\n",
    "|The Silence of the Lambs | {\"silenc\", \"lamb\"}|\n",
    "|Twinkle Twinkle Little Star | {\"twinkl\", \"littl\", \"star\"}|\n",
    "\n",
    "The above table is a good start but the result doesn't represent that there was two \"Twinkle\"s from \"Twinkle Twinkle Little Star\". A better way to do this is with a Document-Term Matrix.\n",
    "\n",
    "This is usually done with a set of documents, known as a corpus (D).\n",
    "\n",
    "#### Document-Term Matrix\n",
    "| | littl |\thous |\tprairi |\tmari |\tlamb |\tsilenc |\ttwinkl |\tstar |\n",
    "|--|--|--|--|--|--|--|--|\n",
    "|Little House on the Prairie\t|1|\t1|\t1|\t0|\t0|\t0|\t0|\t0|\n",
    "|Mary had a Little Lamb\t|1|\t0|\t0|\t1|\t1|\t0|\t0|\t0|\n",
    "|The Silence of the Lambs\t|0|\t0|\t0|\t0|\t1|\t1|\t0|\t0|\n",
    "|Twinkle Twinkle Little Star\t|1|\t0|\t0|\t0|\t0|\t0|\t2|\t1|\n",
    "\n",
    "Each number in the vector is called a term frequency.\n",
    "\n",
    "#### Comparing Documents\n",
    "Compare two documents based on how many words they have in common or how similar their terms frequencies are.\n",
    "|\t|\t                        |littl|\thous|\tprairi|\tmari|\tlamb|\tsilenc|\ttwinkl|\t star|\n",
    "|---|---------------------------|-----|-----|---------|-----|-------|---------|-------|------|\n",
    "|a\t|Little House on the Prairie|\t 1|    1|\t     1|    0|\t   0|\t     0|  \t 0|\t    0|\n",
    "|b\t|Mary had a Little Lamb     |\t 1|\t   0|\t     0|\t   1|\t   1|\t     0|\t     0| \t0|\n",
    "\n",
    "This is done mathematically by a dot product which is the sum of the products of corresponding elements. The larger the dot product will indicate that the two vectors are more similar.\n",
    "\n",
    "![dot](./02_NLP_pipelines/images/01.png)\n",
    "\n",
    "|dot product of a and b =\t|`1*1`\t|`1*0`\t|`1*0`\t|`0*1`\t|`0*1`\t|`0*0`\t|`0*0`\t|`0*0`|\n",
    "|---------------------------|-------|-------|-------|-------|-------|-------|-------|-----|\n",
    "|=\t|1\t|0\t|0|\t0|\t0\t|0\t|0\t|0|\n",
    "|=\t|1\t|   | |  |      |   |   | |\n",
    "\n",
    "The dot product only captures the overlap but doesn't take into account the values that don't overlap. Sometimes this can result in comparing two very different documents leads to a result as documents that are identical.\n",
    "\n",
    "The way to get around this is using cosine similarity. Which still uses the dot product as the numerator but will divide by the products of their magnitudes (Euclidean norms).\n",
    "\n",
    "![docosine](./02_NLP_pipelines/images/02.png)\n",
    "\n",
    "This esentially makes each of the vectors an arrow pointing in a direction and then calculates the theta of the angle made by the arrow of A and B. We can look at this for comparison between a and b in the table below.\n",
    "\n",
    "| `cos(theta) = dot(a, b) / \\|\\|a\\|\\| x \\|\\|b\\|\\|=`\t| `1/3`   |\n",
    "|---------------------------------------------------|---------|\n",
    "|`dot product of a and b =`                         |  `1`    |\n",
    "|`\\|\\|a\\|\\|`\t                                    |`sqrt(3)`|\n",
    "|`\\|\\|b\\|\\|`\t                                    |`sqrt(3)`|\n",
    "\n",
    "Identical documents will have a result of 1 and documents that don't share any similarities will have a result of -1. But documents that share approximately half will result in an orthogonal vector with a result of 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "#### Document Frequency\n",
    "Bag of words treats each words as equally important. But based on our intiuition some words will occur more frequently in a corpus. For example, in financial documents, this corpus may have a high term frequency in terms like cost or price. To compensate for this we can count in how many documents each word occurs.\n",
    "\n",
    "|\t                         |littl\t|hous |\tprairi | mari |\tlamb | silenc |\ttwinkl\t| star |\n",
    "|----------------------------|------|-----|--------|------|------|--------|---------|------|\n",
    "|Little House on the Prairie |\t1\t|1\t  | 1\t   | 0\t  | 0\t | 0\t  |0\t    | 0    |\n",
    "|Mary had a Little Lamb\t     |  1\t|0\t  | 0\t   | 1\t  | 1\t | 0\t  |0\t    | 0    |\n",
    "|The Silence of the Lambs\t |  0\t|0\t  | 0\t   | 0\t  | 1\t | 1\t  |0\t    | 0    |\n",
    "|Twinkle Twinkle Little Star |\t1\t|0\t  | 0\t   | 0\t  | 0\t | 0\t  |2\t    | 1    |\n",
    "|**Document Frequency**      |  3\t|1\t  | 1\t   | 1\t  | 2\t | 1\t  |1\t    | 1    |\n",
    "\n",
    "Then divide the document Frequencies on all the values in the corpus. This now gives a proportional value of the term frequencies but is inversely proportional to how many documents that term appears in.\n",
    "\n",
    "|\t                         |littl\t|hous |\tprairi | mari |\tlamb | silenc |\ttwinkl\t| star |\n",
    "|----------------------------|------|-----|--------|------|------|--------|---------|------|\n",
    "|Little House on the Prairie |\t1/3\t|1\t  | 1\t   | 0\t  | 0\t | 0\t  |0\t    | 0    |\n",
    "|Mary had a Little Lamb\t     |  1/3\t|0\t  | 0\t   | 1\t  | 1/2\t | 0\t  |0\t    | 0    |\n",
    "|The Silence of the Lambs\t |  0\t|0\t  | 0\t   | 0\t  | 1/2\t | 1\t  |0\t    | 0    |\n",
    "|Twinkle Twinkle Little Star |\t1/3\t|0\t  | 0\t   | 0\t  | 0\t | 0\t  |2\t    | 1    |\n",
    "|**Document Frequency**      |  3\t|1\t  | 1\t   | 1\t  | 2\t | 1\t  |1\t    | 1    |\n",
    "\n",
    "Values with a higher value (i.e., \"Mary\" and \"Silence\") are unique to a particular docment while smaller values mean they are frequently used throughout the corpus (i.e., \"Little\" or \"Lamb\"). This allows for better charaterization.\n",
    "\n",
    "#### Term Frequency - Inverse Document Frequency (TF-IDF) Transform\n",
    "Includes two weights:\n",
    "\n",
    "* Term Frequency (tf)\n",
    "* Inverse Document Frequency (idf)\n",
    "\n",
    "#### Term Frequency\n",
    "Is mathematically defined as the count of a term (t) in a document (d) divided by all the terms in the document.\n",
    "\n",
    "![dot](./02_NLP_pipelines/images/03.png)\n",
    "\n",
    "#### Inverse Document Frequency\n",
    "Is the logarithm of the total number of documents in the coprpus (D) divided by the number of documents where the term (t) exists.\n",
    "\n",
    "![dot](./02_NLP_pipelines/images/04.png)\n",
    "\n",
    "#### Resultant Equation of the TF-IDF\n",
    "These come together into the following mathematical formula.\n",
    "\n",
    "![dot](./02_NLP_pipelines/images/05.png)\n",
    "\n",
    "There are many variations that try to smooth or normalize the results or try to prevent edge cases and division by zero errors.\n",
    "\n",
    "But ultimately this is a good way to assign weight to words and indicate their relevance in a given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "As the number of words grows for a given dataset, One-hot encodings becomes less and less sustainable beacuse the size of the word representations grows with the number of words.\n",
    "\n",
    "This is where word embeddings comes in where it limits the word representation to a fixed-size vector. This means for each word we want to find the embedding in a vector space which exhibit desired properties.\n",
    "\n",
    "For example, words with similar meanings such as kid and child should be closer in comparison to words that have disparate meaning (i.e., rock).\n",
    "\n",
    "Another example, are words that are different in similar ways like man, king, woman, and queen. The distance between man and woman should be similar to thew distance between king and queen.\n",
    "\n",
    "For more on word embeddings, take a look at the optional content at the end of the lesson.\n",
    "\n",
    "#### Word Embedding - Word2Vec\n",
    "Word2Vec is one of the most popular used word embeddings. As the name indicates it transforms words into vectors but let's look at how that transformation is done.\n",
    "\n",
    "The core idea is to predict a given word using neighboring words or the using a word to predict neighboring words. This indicates that the model is likely to have a strong grasp of contextual meaning of the words.\n",
    "\n",
    "There are 2 main cases:\n",
    "\n",
    "1. You are given a word and it predicts the neighboring words is called Continuous Skip-gram.\n",
    "2. You are given neigboring words is called continous bag of words (CBoW).\n",
    "\n",
    "##### Case 1: Skip-gram Model\n",
    "In the Skip-gram model, a word is chosen from a sentence. This word is converted into a one-hot encoded vector and fed into a neural network or probabilistic model. The model is designed to predict a few surrounding words, its context. We then would optimize the model's weights or parameters and repeat till it best predicts the surrounding words.\n",
    "\n",
    "Now, take an intermediate representation like a hidden layer in a neural network. The outputs of that layer for a given word become the corresponding word vector.\n",
    "\n",
    "![skip-gram](./02_NLP_pipelines/images/06.PNG)\n",
    "![word2vec](./02_NLP_pipelines/images/07.PNG)\n",
    "\n",
    "##### Case 2: Continuous Bag of Words (CBoW)\n",
    "Yields a very robust representation of words because the meaning of each word is distributed throughout the vector. The size of the word vector dependent on how you want to tune performance versus complexity. Unlike BoW, CBoW's vector size remains constant no matter how many words. Once trained on the traininig set (a large set of word vectors), you can just store them in a lookup table for future use.\n",
    "\n",
    "Now in a look up table it can be used in deep learning architectures. For example, it can be used as the input vector for recurrent neural nets. It is also possible to use RNNs to learn even better word embeddings. Some other optimizations are possible that further reduce the model and training complexity such as representing the output words using Hierarchical Softmax, computing loss using Sparse Cross Entropy, et cetera.\n",
    "\n",
    "#### Global Vectors for Word Representation (GloVe)\n",
    "GloVe or global vectors for word representation is an approach of embedding that tries to directly optimize the vector representation of each word using co-occurrence statistics.\n",
    "\n",
    "First, the probability of a word j appears in the context of word i. For example, what is the probability that the word \"cup\" would be in the context (within 1-2 neighboring words) of the word \"coffee\"? The words \"cup\" and \"coffee\" are often related so we would intuit that it would be have a relatively high probability.\n",
    "\n",
    "Then, we count all such occurrences of i and j in our text collection, and then normalize a count to get a probability. Two random vectors are initialized for each word\n",
    "\n",
    "1. Word as a context\n",
    "2. Word as the target\n",
    "Now, for any pair of words, ij, we want the dot product of their word vectors.\n",
    "\n",
    "![coocurence_prob](./02_NLP_pipelines/images/08.PNG)\n",
    "\n",
    "Using this as our goal and a suitable last function, we can iteratively optimize these word vectors. The result should be a set of vectors that capture the similarities and differences between individual words. If you look at it from another point of view, we are essentially factorizing the co-occurrence probability matrix into two smaller matrices. This is the basic idea behind GloVe. All that sounds good, but why co-occurrence probabilities?\n",
    "\n",
    "Consider this table and probabilities:\n",
    "\n",
    "|       |solid          |\twater         |\n",
    "|-------|---------------|-----------------|\n",
    "|ice    |\tP(solid|ice)|\tP(water|ice)  |\n",
    "|steam\t|P(solid|steam) |\tP(water|steam)|\n",
    "\n",
    "Using our intuition one would come across \"solid\" more often in the context of \"ice\" than \"steam\" and \"water\" could occur in either context with roughly equal probability. And that is what we see in the co-occurance probabilities.\n",
    "\n",
    "![coocurence_prob](./02_NLP_pipelines/images/09.PNG)\n",
    "\n",
    "Given a large corpus, you'll find that the ratio of P solid given ice (P(solid|ice)) to P solid given steam (P(solid|steam)) is much greater than one, while the ratio of P water given ice (P(water|ice)) and P water given steam (P(water|steam))is close to one.\n",
    "\n",
    "Thus, we see that co-occurrence probabilities already exhibit some of the properties we want to capture. In fact, one refinement over using raw probability values is to optimize for the ratio of probabilities. The co-occurence probability matrix is huge and the co-occurrence probability values are typically very low, so it makes sense to work with the log of these values.\n",
    "\n",
    "I encourage you to read the paper that introduced GloVe to get a better understanding of this technique, called [GloVE: Global Vectors for Word Representations](https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "#### Embedding for Deep Learning\n",
    "Word embeddings are fast becoming the de facto choice for representing words, especially for use and deep neural networks. In the distributional hypothesis, states that words that occur in the same contexts tend to have similar meanings. For example, consider these sentences:\n",
    "\n",
    "> A: Would you like to have a cup of <blank>?\n",
    "> B: I like my <blank> black.\n",
    "> C: I need my morning <blank> before I can do anything.\n",
    "\n",
    "By now you probably have a word to fill in the <blank>. Let's look at some follow up questions:\n",
    "\n",
    "1. What would the blank be? \"Tea\" or \"Coffee\"\n",
    "2. What words in the sentence gave you the context clue for the word?\n",
    "    * \"Cup\"\n",
    "    * \"Black\"\n",
    "    * \"Morning\"\n",
    "\n",
    "But it either \"Tea\" or \"Coffee\" could fill in the blanks and make sense. In these contexts, tea and coffee are actually similar. Therefore, when a large collection of sentences is used to learn in embedding, words with common context words tend to get pulled closer and closer together. Of course, there could also be contexts in which tea and coffee are dissimilar.\n",
    "\n",
    "For example:\n",
    "\n",
    "> A: <blank> grounds are great for composting.\n",
    "> B: I prefer loose leaf <blank>.\n",
    "\n",
    "A is clearly talking about \"coffee grounds\". While B is talking about \"loose leaf tea\".\n",
    "\n",
    "We can capture these similarities and differences in the same embedding by adding another dimension. Words can be close along one dimension. For example \"tea\" and \"coffee\" are both breverages but differ in other ways. A dimension could captures all the variability among beverages.\n",
    "\n",
    "In a human language, there are many more dimensions along which word meanings can vary. The more dimensions you can capture in your word vector, the more expressive that representation will be.\n",
    "\n",
    "#### How many dimensions do you really need?\n",
    "For example, a typical neural network architecture designed for an NLP task like word prediction could have a few hundred dimension in a word embedding layer. This might seem large but remember using one-heart encodings is as large as the size of the vocabulary, sometimes in tens of thousands of words.\n",
    "\n",
    "You can also add learning embedding as part of the model training process and obtain a representation that captures the dimensions that are most relevant for your task. This often adds complexity so often we use a pre-trained embeddings (Word2Vec or GloVe) as a look-up unless your use case is very narrow like on for medical terminology. This will allow you to only train the layer specific to your task.\n",
    "\n",
    "Compare this with the network architecture for a computer vision task, say, image classification, the raw input here is also very high dimensional. For example, even 128 by 128 Image contains over 16 thousand pixels. We typically use convolutional layers to exploit the spatial relationships and image data and reduce this dimensionality. Early stages and visual processing are often transferable across tasks, so it is common to use some pre-trained layers from an existing network, like Alex Net or BTG 16 and only learn the later layers. Come to think of it, using an embedding look up for NLP is not on like using pre-treated layers for computer vision. Both are great examples of transfer learning.\n",
    "\n",
    "#### t-SNE\n",
    "Word embeddings need to have high dimensionality to capture sufficient variations in natural language, but this makes them hard to visualize.\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE), is a dimensionality reduction technique that can map high dimensional vectors to a lower dimensional space. It's kind of like Principle Component Analysis (PCA), but it tries to maintain relative distances between objects, so that similar ones stay closer together while dissimilar objects stay further apart.\n",
    "\n",
    "If we look at the larger vector space, we can discover meaningful groups of related words. Sometimes, that takes a while to realize why certain clusters are formed, but most of the groupings are very intuitive.\n",
    "\n",
    "T-SNE also works on other kinds of data, such as images. For example, pictures from the Caltech 101 dataset organized into clusters that roughly correspond to class labels\n",
    "\n",
    "* airplanes with blue sky\n",
    "* sailboats of different shapes and sizes\n",
    "* human faces\n",
    "\n",
    "This is a very useful tool for better understanding the representation that a network learns and for identifying any bugs or other issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "The final stage of the NLP pipeline is **modeling**, which includes designing a statistical or machine learning model, fitting its parameters to training data, using an optimization procedure, and then using it to make predictions about unseen data.\n",
    "\n",
    "The nice thing about working with numerical features is that it allows you to choose from all machine learning models or even a combination of them.\n",
    "\n",
    "Once you have a working model, you can deploy it as a web app, mobile app, or integrate it with other products and services. The possibilities are endless!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Pipeline Recap\n",
    "We covered the 3 major steps of a NLP Pipeline and the ways to approach each of these steps for your given problem.\n",
    "\n",
    "1. Text Processing\n",
    "* Cleaning\n",
    "* Normalization\n",
    "* Tokenization\n",
    "* Stop Word Removal\n",
    "* Part of Speech Tagging\n",
    "* Named Entity Recognition\n",
    "* Stemming and Lemmatization\n",
    "\n",
    "2. Feature Extraction\n",
    "* Bag of Words\n",
    "* TF-IDF\n",
    "* Word Embeddings\n",
    "\n",
    "3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipelines\n",
    "\n",
    "### Introduction\n",
    "Welcome to this lesson on ML Pipelines! Here, we'll cover:\n",
    "\n",
    "* Advantages of Machine Learning Pipelines\n",
    "* Scikit-learn Pipeline\n",
    "* Scikit-learn Feature Union\n",
    "* Pipelines and Grid Search\n",
    "* Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pipeline\n",
    "Below, you'll find a simple example of a machine learning workflow where we generate features from text data using count vectorizer and tf-idf transformer, and then fit it to a random forest classifier. Before we get into using pipelines, let's first use this example to go over some scikit-learn terminology.\n",
    "\n",
    "* **ESTIMATOR**: An estimator is any object that learns from data, whether it's a classification, regression, or clustering algorithm, or a transformer that extracts or filters useful features from raw data. Since estimators learn from data, they each must have a `fit` method that takes a dataset. In the example below, the `CountVectorizer`, `TfidfTransformer`, and `RandomForestClassifier` are all estimators, and each have a `fit` method.\n",
    "* **TRANSFORMER**: A transformer is a specific type of estimator that has a `fit` method to learn from training data, and then a `transform` method to apply a transformation model to new data. These transformations can include cleaning, reducing, expanding, or generating features. In the example below, `CountVectorizer` and `TfidfTransformer` are transformers.\n",
    "* **PREDICTOR**: A predictor is a specific type of estimator that has a `predict` method to predict on test data based on a supervised learning algorithm, and has a `fit` method to train the model on training data. The final estimator, `RandomForestClassifier`, in the example below is a predictor.\n",
    "\n",
    "In machine learning tasks, it's pretty common to have a very specific sequence of transformers to fit to data before applying a final estimator, such as this classifier. And normally, we'd have to initialize all the estimators, `fit` and `transform` the training data for each of the transformers, and then fit to the final estimator. Next, we'd have to call `transform` for each transformer again to the test data, and finally call `predict` on the final estimator.\n",
    "\n",
    "#### Without pipeline:\n",
    "\n",
    "```python\n",
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# train classifier\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predict on test data\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "```\n",
    "\n",
    "Fortunately, you can actually automate all of this fitting, transforming, and predicting, by chaining these estimators together into a single estimator object. That single estimator would be **scikit-learn's Pipeline**. To create this pipeline, we just need a list of `(key, value)` pairs, where the key is a string containing what you want to name the step, and the value is the estimator object.\n",
    "\n",
    "#### Using pipeline:\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# evaluate all steps on test set\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "\n",
    "Now, by fitting our pipeline to the training data, we're accomplishing exactly what we would by fitting and transforming each of these steps to our training data one by one. Similarly, when we call `predict` on our pipeline to our test data, we're accomplishing what we would by calling `transform` on each of our transformer objects to our test data and then calling `predict` on our final estimator. Not only does this make our code much shorter and simpler, it has other great advantages, which we'll cover in the next video.\n",
    "\n",
    "Note that every step of this pipeline has to be a transformer, except for the last step, which can be of an estimator type. Pipeline takes on all the methods of whatever the last estimator in its sequence is. For example, here, since the final estimator of our pipeline is a classifier, the pipeline object can be used as a classifier, taking on the `fit` and `predict` methods of its last step. Alternatively, if the last estimator was a transformer, then pipeline would be a transformer.\n",
    "\n",
    "#### Advantages of Using Pipeline\n",
    "Below are two videos explaining the advantages of using scikit-learn's Pipeline as seen in the previous video.\n",
    "\n",
    "1. Simplicity and Convencience\n",
    "* **Automates repetitive steps** - Chaining all of your steps into one estimator allows you to fit and predict on all steps of your sequence automatically with one call. It handles smaller steps for you, so you can focus on implementing higher level changes swiftly and efficiently.\n",
    "* **Easily understandable workflow** - Not only does this make your code more concise, it also makes your workflow much easier to understand and modify. Without Pipeline, your model can easily turn into messy spaghetti code from all the adjustments and experimentation required to improve your model.\n",
    "* **Reduces mental workload** - Because Pipeline automates the intermediate actions required to execute each step, it reduces the mental burden of having to keep track of all your data transformations. Using Pipeline may require some extra work at the beginning of your modeling process, but it prevents a lot of headaches later on.\n",
    "\n",
    "2. Optimizing Entire Workflow\n",
    "* **GRID SEARCH**: Method that automates the process of testing different hyper parameters to optimize a model.\n",
    "* By running grid search on your pipeline, you're able to optimize your entire workflow, including data transformation and modeling steps. This accounts for any interactions among the steps that may affect the final metrics.\n",
    "* Without grid search, tuning these parameters can be painfully slow, incomplete, and messy.\n",
    "\n",
    "3. Preventing Data leakage\n",
    "* Using Pipeline, all transformations for data preparation and feature extractions occur within each fold of the cross validation process.\n",
    "* This prevents common mistakes where you’d allow your training process to be influenced by your test data - for example, if you used the entire training dataset to normalize or extract features from your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines and Feature Unions\n",
    "* **FEATURE UNION**: [Feature union](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) is a class in scikit-learn’s Pipeline module that allows us to perform steps in parallel and take the union of their results for the next step.\n",
    "* A **pipeline** performs a list of steps in a linear sequence, while a feature union performs a list of steps in parallel and then combines their results.\n",
    "* In more complex workflows, multiple feature unions are often used within pipelines, and multiple pipelines are used within feature unions.\n",
    "\n",
    "#### Using Feature Union\n",
    "Taking the example from the previous video, let's say you wanted to extract two different kinds of features from the same text column - tfidf values, and the length of the text. Your first approach might be to create an additional column from the `text` column called `text_length` like this. Then both `text` and `text_length` can be part of your feature matrix. But now your pipeline would break. You can't run `CountVectorizer` on NumPy arrays of strings and integers.\n",
    "\n",
    "```python\n",
    "df['txt_length'] = df['text'].apply(len)\n",
    "X = df[['text', 'txt_length']].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "\n",
    "Let's say you had a custom transformer called `TextLengthExtractor`. Now, you could leave `X_train` as just the original text column, if you could figure out how to add the text length extractor to your pipeline. If only you could fit it on the original text data, rather than the output of the previous transformer. But you need both the outputs of `TfidfTransformer` and `TextLengthExtractor` to feed into the classifier as input.\n",
    "\n",
    "```python\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('txt_length', TextLengthExtractor()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "* Feature unions are super helpful for handling these situations, where we need to run two steps in parallel on the same data and combine their results to pass into the next step.\n",
    "* Like pipelines, feature unions are built using a list of `(key, value)` pairs, where the key is the string that you want to name a step, and the value is the estimator object. Also like pipelines, feature unions combine a list of estimators to become a single estimator. However, a feature union runs its estimators in parallel, rather than in a sequence as a pipeline does. In this example, the estimators run in parallel are `nlp_pipeline` and `text_length`. Notice we use a pipeline in this feature union to make sure the count vectorizer and tfidf transformer steps are still running in sequence.\n",
    "\n",
    "```python\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('nlp_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer()\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "\n",
    "        ('txt_len', TextLengthExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "* Now, our pipeline doesn't break and uses both features! This would be equivalent to this code.\n",
    "\n",
    "```python\n",
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "txt_len = TextLengthExtractor()\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# train classifier\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "\n",
    "X_train_len = txt_len.fit_transform(X_train)\n",
    "X_train_features = hstack([X_train_tfidf, X_train_len])\n",
    "clf.fit(X_train_features, y_train)\n",
    "\n",
    "# predict on test data\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "\n",
    "X_test_len = txt_len.transform(X_test)\n",
    "X_test_features = hstack([X_test_tfidf, X_test_len])\n",
    "y_pred = clf.predict(X_test_features)\n",
    "```\n",
    "* The tfidf transformer and the text length extractor are fit to the input data, in this case the raw data, independently. They are then performed in parallel, and their outputs are combined and passed to the next estimator, in this case, the classifier.\n",
    "\n",
    "Read more about feature unions in Scikit-learn's [user guide](http://scikit-learn.org/stable/modules/pipeline.html#feature-union).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
