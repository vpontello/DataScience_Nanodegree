{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering\n",
    "\n",
    "**Data engineers** gather data from various sources, process, combine and manipulate the data so it can be easily accessed and used. This whole process from gathering data to getting it ready to use can be automated by software programs, also called **data pipelines**.\n",
    "\n",
    "Data engineers are responsible for gather data from various sources, process, and combine and the data so it can be easily accessed and used. This process is often automated by data pipelines.\n",
    "\n",
    "As a data scientist, it's important to have some level of skill in data engineering despite the size of the company you work in.\n",
    "\n",
    "In larger companies, although there are dedicated data engineers, data scientists need to communicate with data engineers about the data they need for the data science project. In smaller companies, data scientists may need to take some of the responsibility of a data engineer.\n",
    "\n",
    "## Data Pipelines: ETL vs ELT\n",
    "Data pipeline is a generic term for moving data from one place to another. For example, it could be moving data from one server to another server.\n",
    "\n",
    "### ETL\n",
    "An ETL pipeline is a specific kind of data pipeline and very common. ETL stands for Extract, Transform, Load. Imagine that you have a database containing web log data. Each entry contains the IP address of a user, a timestamp, and the link that the user clicked.\n",
    "\n",
    "What if your company wanted to run an analysis of links clicked by city and by day? You would need another data set that maps IP address to a city, and you would also need to extract the day from the timestamp. With an ETL pipeline, you could run code once per day that would extract the previous day's log data, map IP address to city, aggregate link clicks by city, and then load these results into a new database. That way, a data analyst or scientist would have access to a table of log data by city and day. That is more convenient than always having to run the same complex data transformations on the raw web log data.\n",
    "\n",
    "Before cloud computing, businesses stored their data on large, expensive, private servers. Running queries on large data sets, like raw web log data, could be expensive both economically and in terms of time. But data analysts might need to query a database multiple times even in the same day; hence, pre-aggregating the data with an ETL pipeline makes sense.\n",
    "\n",
    "### ELT\n",
    "ELT (Extract, Load, Transform) pipelines have gained traction since the advent of cloud computing. Cloud computing has lowered the cost of storing data and running queries on large, raw data sets. Many of these cloud services, like Amazon Redshift, Google BigQuery, or IBM Db2 can be queried using SQL or a SQL-like language. With these tools, the data gets extracted, then loaded directly, and finally transformed at the end of the pipeline.\n",
    "\n",
    "However, ETL pipelines are still used even with these cloud tools. Oftentimes, it still makes sense to run ETL pipelines and store data in a more readable or intuitive format. This can help data analysts and scientists work more efficiently as well as help an organization become more data driven.\n",
    "\n",
    "## World Bank Data\n",
    "This lesson uses data from the World Bank. The data comes from two sources:\n",
    "\n",
    "1. [World Bank Indicator Data](https://data.worldbank.org/indicator) - This data contains socio-economic indicators for countries around the world. A few example indicators include population, arable land, and central government debt.\n",
    "2. [World Bank Project Data](https://datacatalog.worldbank.org/dataset/world-bank-projects-operations) - This data set contains information about World Bank project lending since 1947.\n",
    "\n",
    "Both of these data sets are available in different formats including as a csv file, json, or xml. You can download the csv directly or you can use the World Bank APIs to extract data from the World Bank's servers. You'll be doing both in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the data file types you'll work with\n",
    "### CSV files\n",
    "CSV stands for comma-separated values. These types of files separate values with a comma, and each entry is on a separate line. Oftentimes, the first entry will contain variable names. Here is an example of what CSV data looks like. This is an abbreviated version of the first three lines in the World Bank projects data csv file.\n",
    "```\n",
    "id,regionname,countryname,prodline,lendinginstr\n",
    "P162228,Other,World;World,RE,Investment Project Financing\n",
    "P163962,Africa,Democratic Republic of the Congo;Democratic Republic of the Congo,PE,Investment Project Financing\n",
    "```\n",
    "### JSON\n",
    "JSON is a file format with key/value pairs. It looks like a Python dictionary. The exact same CSV file represented in JSON could look like this:\n",
    "```json\n",
    "[{\"id\":\"P162228\",\"regionname\":\"Other\",\"countryname\":\"World;World\",\"prodline\":\"RE\",\"lendinginstr\":\"Investment Project Financing\"},{\"id\":\"P163962\",\"regionname\":\"Africa\",\"countryname\":\"Democratic Republic of the Congo;Democratic Republic of the Congo\",\"prodline\":\"PE\",\"lendinginstr\":\"Investment Project Financing\"},{\"id\":\"P167672\",\"regionname\":\"South Asia\",\"countryname\":\"People\\'s Republic of Bangladesh;People\\'s Republic of Bangladesh\",\"prodline\":\"PE\",\"lendinginstr\":\"Investment Project Financing\"}]\n",
    "```\n",
    "Each line in the data is inside of a squiggly bracket {}. The variable names are the keys, and the variable values are the values.\n",
    "\n",
    "There are other ways to organize JSON data, but the general rule is that JSON is organized into key/value pairs. For example, here is a different way to represent the same data using JSON:\n",
    "```json\n",
    "{\"id\":{\"0\":\"P162228\",\"1\":\"P163962\",\"2\":\"P167672\"},\"regionname\":{\"0\":\"Other\",\"1\":\"Africa\",\"2\":\"South Asia\"},\"countryname\":{\"0\":\"World;World\",\"1\":\"Democratic Republic of the Congo;Democratic Republic of the Congo\",\"2\":\"People\\'s Republic of Bangladesh;People\\'s Republic of Bangladesh\"},\"prodline\":{\"0\":\"RE\",\"1\":\"PE\",\"2\":\"PE\"},\"lendinginstr\":{\"0\":\"Investment Project Financing\",\"1\":\"Investment Project Financing\",\"2\":\"Investment Project Financing\"}}\n",
    "```\n",
    "### XML\n",
    "Another data format is called XML (Extensible Markup Language). XML is very similar to HTML at least in terms of formatting. The main difference between the two is that HTML has pre-defined tags that are standardized. In XML, tags can be tailored to the data set. Here is what this same data would look like as XML.\n",
    "\n",
    "```xml\n",
    "<ENTRY>\n",
    "  <ID>P162228</ID>\n",
    "  <REGIONNAME>Other</REGIONNAME>\n",
    "  <COUNTRYNAME>World;World</COUNTRYNAME>\n",
    "  <PRODLINE>RE</PRODLINE>\n",
    "  <LENDINGINSTR>Investment Project Financing</LENDINGINSTR>\n",
    "</ENTRY>\n",
    "<ENTRY>\n",
    "  <ID>P163962</ID>\n",
    "  <REGIONNAME>Africa</REGIONNAME>\n",
    "  <COUNTRYNAME>Democratic Republic of the Congo;Democratic Republic of the Congo</COUNTRYNAME>\n",
    "  <PRODLINE>PE</PRODLINE>\n",
    "  <LENDINGINSTR>Investment Project Financing</LENDINGINSTR>\n",
    "</ENTRY>\n",
    "<ENTRY>\n",
    "  <ID>P167672</ID>\n",
    "  <REGIONNAME>South Asia</REGIONNAME>\n",
    "  <COUNTRYNAME>People's Republic of Bangladesh;People's Republic of Bangladesh</COUNTRYNAME>\n",
    "  <PRODLINE>PE</PRODLINE>\n",
    "  <LENDINGINSTR>Investment Project Financing</LENDINGINSTR>\n",
    "</ENTRY>\n",
    "```\n",
    "\n",
    "ML is falling out of favor especially because JSON tends to be easier to navigate; however, you still might come across XML data. The World Bank API, for example, can return either XML data or JSON data. From a data perspective, the process for handling HTML and XML data is essentially the same.\n",
    "\n",
    "### SQL databases\n",
    "SQL databases store data in tables using [primary and foreign keys](https://docs.microsoft.com/en-us/sql/relational-databases/tables/primary-and-foreign-key-constraints?view=sql-server-2017). In a SQL database, the same data would look like this:\n",
    "\n",
    "|id\t| regionname\t|countryname|\tprodline|\tlendinginstr |\n",
    "|---|-------------|-----------|---------|--------------|\n",
    "|P162228\t|Other\t|World;World\t|RE\t|Investment Project Financing|\n",
    "|P163962\t|Africa\t|Democratic Republic of the Congo;Democratic Republic of the Congo\t|PE\t|Investment Project Financing|\n",
    "|P167672\t|South Asia|\tPeople's Republic of Bangladesh;People's Republic of Bangladesh\t|PE|\tInvestment Project Financing|\n",
    "\n",
    "### Text Files\n",
    "This course won't go into much detail about text data. There are other Udacity courses, namely on natural language processing, that go into the details of processing text for machine learning.\n",
    "\n",
    "Text data present their own issues. Whereas CSV, JSON, XML, and SQL data are organized with a clear structure, text is more ambiguous. For example, the World Bank project data country names are written like this\n",
    "```\n",
    "Democratic Republic of the Congo;Democratic Republic of the Congo\n",
    "```\n",
    "In the World Bank Indicator data sets, the Democratic Republic of the Congo is represented by the abbreviation \"Congo, Dem. Rep.\" You'll have to clean these country names to join the data sets together.\n",
    "\n",
    "### Extracting Data from the Web\n",
    "In this lesson, you'll see how to extract data from the web using an APIs (Application Programming Interface). APIs generally provide data in either JSON or XML format.\n",
    "\n",
    "Companies and organizations provide APIs so that programmers can access data in an official, safe way. APIs allow you to download, and sometimes even upload or modify, data from a web server without giving you direct access.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "Transforming data means getting data ready for a machine learning algorithm or other data science projects. Transforming data can involve a wide range of processes such as combining data from different sources, cleaning the data, engineering new features. Hence, you transform the original datasets to create a new dataset that is ready for use.\n",
    "\n",
    "### Combining Data\n",
    "\n",
    "Oftentimes you need to combine data from different datasets. The process of combining data can be complicated and very different from case to case.\n",
    "\n",
    "Datasets can be from different sources and have different content so you need to check the data closely before combining them. Or they may be in different formats meaning you will need to transform data from one format to another first.\n",
    "\n",
    "In the next part of the lesson, you will have an exercise for combining data from different sources using the Python Pandas package. If you are unfamiliar with the Pandas, you can review the learning resources below to prepare yourself before heading to the exercise.\n",
    "\n",
    "### Cleaning Data\n",
    "\n",
    "Dirty data refers to data that contains errors. Data error can come from many sources including:\n",
    "\n",
    "* data entry mistakes\n",
    "* duplicate data\n",
    "* incomplete records\n",
    "* inconsistencies between dataset\n",
    "\n",
    "It's very important to audit data after obtaining it, otherwise, the data science projects will not perform well or even worse, give you wrong results.\n",
    "\n",
    "In the next section, you will have an exercise on data cleaning. Your job is to clean the data so the country names across different datasets are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "In the video, I say that a machine learning algorithm won't work with missing values. This is essentially correct; however, there are a couple of situations where this isn't quite true. For example, if you had a categorical variable, you could keep the NULL value as one of the options.\n",
    "\n",
    "Like if theme_2 could have a value of agriculture, banking, or NULL, you might encode this variable as 0, 1, 2 where the value 2 stands in for the NULL value. You could do something similar for one-hot encoding where the theme_2 variable becomes 3 true/false features: theme_2_agriculture, theme_2_banking, theme_2_NULL. You could have to make sure that this improves your model performance.\n",
    "\n",
    "There are also implementations of some machine learning algorithms, such as [gradient boosting](https://xgboost.readthedocs.io/en/latest/) decision trees that can [handle missing values](https://github.com/dmlc/xgboost/issues/21).\n",
    "\n",
    "### Missing Data - Delete\n",
    "There are two ways to handle missing values:\n",
    "\n",
    "* Delete data\n",
    "* Fill missing values (also called imputation)\n",
    "\n",
    "You can delete a feature with a large percentage of missing values because it's unlikely to contribute to your machine learning model unless somehow you can find the missing values from another source. If you think deleting a row consisting of a lot of missing values won't affect your result, you can also delete it.\n",
    "\n",
    "However, deleting missing values is not the only option. In the next part, we will talk about another option - fill in missing values.\n",
    "\n",
    "### Missing Data - Inpute\n",
    "The process of filling in missing values is called `imputation`. Some of the imputation techniques are mean substitution, forward fill, and backward fill.\n",
    "\n",
    "##### Mean Substitution\n",
    "The mean substitution method fills in the missing values using the `column mean`. You can even group the data first then filling the data by the means in different groups. Alternatively, you can fill in the missing values by `median` or `mode`.\n",
    "\n",
    "##### Forward Fill or Backward Fill\n",
    "Forward fill or backward fill works for `ordered` or `times series` data. In both methods, you can use neighboring cells to fill in the missing value.\n",
    "\n",
    "To use these methods, you should always make sure your data is `sorted` by a timestamp or in a meaningful way. With the forward fill, values are pushed forward `down` to replace any missing values. With backward fill, values move `up` to fill missing values.\n",
    "\n",
    "## Duplicate Data\n",
    "Data duplication is obvious when the same row shows up more than once and you can simply remove the duplicate data using the Pandas drop duplicates method. However, duplicate data can sometimes be trickier to find which requires you to comb through the data to recognize and eliminate duplications.\n",
    "\n",
    "## Dummy Variables\n",
    "\n",
    "### When to Remove a Feature\n",
    "As mentioned in the video, if you have five categories, you only really need four features. For example, if the categories are \"agriculture\", \"banking\", \"retail\", \"roads\", and \"government\", then you only need four of those five categories for dummy variables. This topic is somewhat outside the scope of a data engineer.\n",
    "\n",
    "In some cases, you don't necessarily need to remove one of the features. It will depend on your application. In regression models, which use linear combinations of features, removing a dummy variable is important. For a decision tree, removing one of the variables is not needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "### Outliers - How to Find Them\n",
    "**Outliers** are data points that are far away from the rest of the data. Outliers can be caused by errors such as data entry errors or processing mistakes. However, outliers can also be legitimate data.\n",
    "\n",
    "There are some methods to detect outliers:\n",
    "\n",
    "**Data visualization**\n",
    "When working with one or two-dimensional data, you can visualize data to detect outliers. The data points far away from the rest of the data on the plot can potentially be outliers.\n",
    "\n",
    "**Statistical methods**\n",
    "Statistical properties of data like means, standard deviations, quantiles can be used to identify outliers, for example, the z-score from the norma distribution and the Tukey method.\n",
    "\n",
    "**Machine learning methods**\n",
    "When it comes to high-dimensional data, you can use machine learning techniques such as PCA to reduce the data dimensions. Then you can use the methods discussed before to detect outliers.\n",
    "\n",
    "Another way is to cluster the data then calculate the distance from each data point to the cluster centroid. A large distance may indicate an outlier.\n",
    "\n",
    "Next, you will get some practice on identifying outliers.\n",
    "\n",
    "#### Outlier Detection Resources [Optional]\n",
    "Here are a couple of links to outlier detection processes and algorithms. Since this is an ETL course rather than a statistics course, you don't need to read these in order to complete the lesson.\n",
    "\n",
    "* [scikit-learn novelty and outlier detection](http://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "* [statistical and machine learning methods for outlier detection](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561)\n",
    "\n",
    "#### Tukey Rule\n",
    "* Find the first quartile (ie .25 quantile)\n",
    "* Find the third quartile (ie .75 quantile)\n",
    "* Calculate the inter-quartile range (Q3 - Q1)\n",
    "* Any value that is greater than Q3 + 1.5 * IQR is an outlier\n",
    "* Any value that is less than Qe - 1.5 * IQR is an outlier\n",
    "\n",
    "### Outliers - What to do\n",
    "Now you've identified the outliers then what next? The answer varies depending on how the outlier affects your machine learning model. If removing the outlier improves your machine learning model, maybe it's best to delete the outliers. But if the outlier has a special meaning so it has to be taken into account in your model, then it's best to keep it or find other solutions. In another case, if removing the outlier has little or no effect on your results, you can leave the outliers as it.\n",
    "\n",
    "## Scaling Data\n",
    "Numerical data comes in all different distribution patterns and ranges. This can be an issue for machine learning algorithms that calculate Euclidean distance between points, such as PCA, linear regression with gradient descent. This issue can be solved by performing data **normalization** or **feature scaling**. Two common ways to scale features are **rescaling** and **standardization**.\n",
    "\n",
    "### Rescaling / Normalization\n",
    "With rescaling, the distribution of the data remains the same but the range changes to 0-1. You scale down the data range so the minimum value is zero and the maximum value is one.\n",
    "\n",
    "To normalize data, you take a feature, like gdp, and use the following formula\n",
    "\n",
    "$x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "where \n",
    "* x is a value of gdp\n",
    "* x_max is the maximum gdp in the data\n",
    "* x_min is the minimum GDP in the data\n",
    "\n",
    "\n",
    "### Standardization\n",
    "With standardization, the general shape of the distribution remains the same but the mean and standard deviation are standardized. You transform the data so that it has a mean of zero and a standard deviation of one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
