{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization for Recommendation Engines\n",
    "\n",
    "## What's Ahead\n",
    "In this lesson, you will learn about three main topics:\n",
    "\n",
    "1. We will look from a high level at how you might go about validating your recommendations.\n",
    "2. We will look at matrix factorization as a method to use machine learning to make recommendations.\n",
    "3. We will look at combining recommendation techniques to make predictions to existing and new users and for existing and new items, otherwise known as the Cold Start Problem, as you can see in the graphic below\n",
    "\n",
    "As we go through this lesson, you will come to realize that there are a lot of difficulties in working with recommendation engines which make them still an exciting field to study! This is especially true when you combine your recommendations with a specific product type.\n",
    "\n",
    "Recommending movies, recommending restaurants, or recommending clothing might happen in a number of different ways. However, the techniques you will learn in this lesson are often extendable to any of these cases.\n",
    "\n",
    "![01](images/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Know Our Recommendations Are Good?\n",
    "\n",
    "### Training and Testing Data For Recommendations\n",
    "In the last lesson, you were making recommendations by providing a list of popular items, or a list of items that the user hadn't observed but that someone with similar tastes had observed. However, understanding if these recommendations are good in practice means that you have to deploy these recommendations to users and see how it impacts your metrics (sales, higher engagement, clicks, conversions, etc.).\n",
    "\n",
    "You may not want your recommendations to go live to understand how well they work. In these cases, you will want to split your data into training and testing portions. In these cases, you can train your recommendation engine on a subset of the data, then you can test how well your recommendation engine performs on a test set of data before deploying your model to the world.\n",
    "\n",
    "However, the cases you saw in the last lesson, where just a list of recommendations was provided, don't actually lend themselves very well to training and testing methods of evaluation. In the next upcoming pages, you will be introduced to matrix factorization, which actually does work quite well for these situations.\n",
    "\n",
    "### Validating Your Recommendations\n",
    "\n",
    "#### Online Testing\n",
    "For online methods of testing a recommender's performance, many of the methods you saw in the previous lesson work very well - you can deploy your recommendations and just watch your metrics carefully. It is common in practice to set up online recommendations to have an \"old\" version of recommended items, which is compared to a new page that uses a new recommendation strategy.\n",
    "\n",
    "All ideas associated with A/B testing that you learned in the earlier lessons are critical to watching your metrics in online learning, and ultimately, choosing a recommendation strategy that works best for your products and customers.\n",
    "\n",
    "#### Offline Testing\n",
    "In many cases, a company might not let you simply deploy your recommendations out into the real world any time you feel like it. Testing out your recommendations in a training-testing environment prior to deploying them is called **offline** testing.\n",
    "\n",
    "The recommendation methods you built in the previous lesson actually don't work very well for offline testing. In offline testing, it is ideal to not just obtain a list of recommendations for each individual, because we ultimately don't know if a user doesn't use an item because they don't like it, or because they just haven't used it yet (but would like it). Rather, it would be great if we have an idea of how much each user would like each item using a predicted rating. Then we can compare this predicted rating to the actual rating any individual gives to an item in the future.\n",
    "\n",
    "In the previous video, you saw an example of a user to whom we gave a list of movies that they still hadn't seen. Therefore, we couldn't tell how well we were doing with our recommendations. Techniques related to matrix factorization lend themselves nicely to solving this problem.\n",
    "\n",
    "#### User Groups\n",
    "The final (possible) method of validating your recommendations is by having user groups give feedback on items you would recommend for them. Obtaining good user groups that are representative of your customers can be a challenge on its own. This is especially true when you have a lot of products and a very large consumer base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "In the next part of this lesson, you will first get exposure to Singular Value Decomposition or SVD. We will soon see why this technique falls short for many recommendation problems. However, understanding traditional SVD approaches to matrix factorization is useful as a start to a number of matrix factorization techniques that are possible in practice.\n",
    "\n",
    "In order to implement SVD for many recommendation engines, we will need to use a slightly modified approach known as FunkSVD. This approach proved to work incredibly well during the [Netflix competition](https://en.wikipedia.org/wiki/Netflix_Prize), and therefore, it is one of the most popular recommendation approaches in use today.\n",
    "\n",
    "### Latent Factors\n",
    "When performing SVD, we create a matrix of users by items (or customers by movies in our specific example), with user ratings for each item scattered throughout the matrix. An example is shown in the image below.\n",
    "\n",
    "![02](images/02.png)\n",
    "\n",
    "You can see that this matrix doesn't have any specific information about the users or items. Rather, it just holds the ratings that each user gave to each item. Using SVD on this matrix, we can find latent features related to the movies and customers. This is amazing because the dataset doesn't contain any information about the customers or movies!\n",
    "\n",
    "> Latent factors are features that aren't actually observed in the data, but can be inferred based on the relationships that occur\n",
    "\n",
    "### How does SVD actually works\n",
    "Let's do a quick check of understanding. If we let AA be our user-item matrix, we can write the decomposition of that matrix in the following way.\n",
    "\n",
    "![03](images/03.PNG)\n",
    "![06](images/06.PNG)\n",
    "![04](images/04.PNG)\n",
    "![05](images/05.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Closed-Form Solution\n",
    "\n",
    "#### What Is A Closed-Form Solution?\n",
    "A closed-form solution is one where you can directly find the solution values (unlike iterative solutions, which are commonly used in practice). There isn't an iterative approach to solving a particular equation. One of the most popular examples of a closed-form solution is the solution for multiple linear regression. That is if we want to find an estimate for \\betaÎ² in the following situation:\n",
    "$$\n",
    "y = X\\beta\n",
    "$$\n",
    "We can find it by computing the **Best Linear Unbiased Estimate (BLUE)**. It can be found **in closed form** using the equation:\n",
    "$$\n",
    "\\hat{\\beta} = (X'X) - X'y\n",
    "$$\n",
    "where **X** is a matrix of explanatory inputs and **y** is a response vector.\n",
    "\n",
    "Another common example of a closed-form solution is the quadratic equation. If we want to find x that solves:\n",
    "$$\n",
    "ax^2 + bx + c =0\n",
    "$$\n",
    "We can find these values using the quadratic formula:\n",
    "$$\n",
    "x = \\frac{-b+-\\sqrt{b^2-4ac}}{2a}\n",
    "$$\n",
    "**Each of these is an example of a closed-form solution because in each case we have an equation that allows us to solve directly for our values of interest.**\n",
    "\n",
    "#### Closed-Form Solutions for SVD\n",
    "It turns out there is a closed-form solution for Singular Value Decomposition that can be used to identify each of the matrices of interest $(U, \\Sigma, V)$. The most straightforward explanation of this closed-form solution can be found at [this MIT link](http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm).\n",
    "\n",
    "> \"Calculating the SVD consists of finding the eigenvalues and eigenvectors of $AA'$ and $A'A$. The eigenvectors of $A'A$ make up the columns of $V$, the eigenvectors of $AA'$ make up the columns of $U$. Also, the singular values in $\\Sigma$ are square roots of eigenvalues from $AA'$ or $A'A$. The singular values are the diagonal entries of the $\\sigma$ matrix and are arranged in descending order. The singular values are always real numbers.If the matrix $A$ is a real matrix, then $U$ and $V$ are also real.\"\n",
    "\n",
    "Again, you can see a fully worked example of the closed-form solution at the [MIT Link here](http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm).\n",
    "\n",
    "#### A More Common Approach\n",
    "The main issue with the closed-form solution (especially for us) is that it doesn't actually work when we have missing data. Instead, Simon Funk (and then many followers) came up with other solutions for finding our matrices of interest in these cases using **gradient descent**.\n",
    "\n",
    "So all of this is to say, people don't really use the closed-form solution for SVD, and therefore, we aren't going to spend a lot of time on it either. The link above is all you need to know. Now, we are going to look at the main way that the matrices in SVD are estimated, as this is what is used for estimating values in FunkSVD.\n",
    "\n",
    "#### Additional Resources\n",
    "Below are some additional resources in case you are looking for others that go beyond what was shown in the simplified MIT paper.\n",
    "* [Stanford Discussion on SVD](http://infolab.stanford.edu/~ullman/mmds/ch11.pdf)\n",
    "    * Read the \"Dimensionality Reduction\" chapter from a Stanford University textbook. The goal of dimensionality reduction is to replace a large matrix with two or more other matrices whose sizes are much smaller than the original, but from which the original can be approximately reconstructed, usually by taking their product. Topics covered are: Eigenvalues and Eigenvectors, Finding Eigenpairs by Power Iteration, Principal-Component Analysis, Dimensionality Reduction by PCA, Singular-Value Decomposition, Concepts, Queries Using the Singular-Value Decomposition, Using SVD for Dimensionality Reduction, Decomposing Sparse Matrices, and CUR Decomposition\n",
    "* [Why are Singular Values Always Positive on StackExchange](https://math.stackexchange.com/questions/2060572/why-are-singular-values-always-non-negative)\n",
    "    * Read the top 4 answers to this question of math.stackexchange.com\n",
    "* [An additional resource for SVD in Python](https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/)\n",
    "    * Use this developer toolkit online textbook called Machine Learning Mastery. While it is a general book covering lots of topics, it is targeted towards the developer audience.\n",
    "* [Using Missing Values to Improve Recommendations in SVD](https://www.hindawi.com/journals/mpe/2015/380472/)\n",
    "    * In this white paper \"Improving Top-N Recommendation Performance Using Missing Data\", read about a different approach to missing data. \"Most studies consider missing data as unknown information and only use observed data to learn models and generate recommendations. However, data are missing not at random. Part of the missing data is due to the fact that users choose not to rate them. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition Takeaways\n",
    "Three main takeaways from the previous notebook:\n",
    "\n",
    "1. The latent factors retrieved from SVD aren't actually labeled.\n",
    "2. We can get an idea of how many latent factors we might want to keep by using the Sigma matrix.\n",
    "3. SVD in NumPy will not work when our matrix has missing values. **This makes this technique less than useful for our current user-movie matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\sigma \\Sigma $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FunkSVD\n",
    "\n",
    "### The algorithm\n",
    "This technique is especially useful for matrices with lots of missing values. We can actually use the values we do know to help compute the latent values.\n",
    "\n",
    "The general algorithm is as follows\n",
    "\n",
    "* Place random vales in both matrixes\n",
    "* Then in order to calculate a rating prediction, we take a row from the latent array and a column from the movie array. The error is the difference between the predicted and the actual. The ultimate goal is to minimize this error. We do this by changing the weights in each matrix using gradient descent.\n",
    "* In each matrix, we take the derivative of the error with respect to each value\n",
    "* The chain rule allows us to find the direction to move towards minimization of each matrix\n",
    "* The new values replace the current values of the matrix\n",
    "* We iterate until our error is within an acceptable range,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cold Start Problem\n",
    "The **cold start problem** is the problem that new users and new items to a platform don't have any ratings. Because these users and items don't have any ratings, it is impossible to use collaborative filtering methods to make recommendations.\n",
    "\n",
    "Therefore, methods you used in the previous lesson like (rank-based and content-based recommenders) are the only way to get started with making recommendations for these individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this lesson, you got your hands on some of the most important ideas associated with recommendation systems:\n",
    "### Recommender Validation\n",
    "You looked at methods for validating your recommendations (when possible) using offline methods. In these cases, you could split your data into training and testing data. Frequently this split is based on time, where events earlier in time are in the training data, and events later in time are in a testing dataset.\n",
    "\n",
    "We also quickly introduced the idea of being able to see how well your recommendation engine works by simply throwing it out into the world to directly see the impact.\n",
    "\n",
    "### Matrix Factorization with SVD\n",
    "Next, we looked at matrix factorization as a technique for making recommendations. Traditional singular value decomposition a technique that can be used when your matrices have no missing values. In this decomposition technique, a user-item (A) can be decomposed as follows:\n",
    "$$\n",
    "A = U\\Sigma V^T \n",
    "$$\n",
    "Where\n",
    "\n",
    "* $U$ gives information about how users are related to latent features.\n",
    "* $\\Sigma$ gives information about how many latent features matter towards recreating the user-item matrix.\n",
    "* $V^T$ gives information about how much each movie is related to latent features.\n",
    "\n",
    "Since this traditional decomposition doesn't actually work when our matrices have missing values, we looked at another method for decomposing matrices.\n",
    "\n",
    "### FunkSVD\n",
    "FunkSVD was a new method that you found to be useful for matrices with missing values. With this matrix factorization you decomposed a user-item (A) as follows:\n",
    "$$\n",
    "A = UV^T \n",
    "$$\n",
    "Where\n",
    "* $U$ gives information about how users are related to latent features.\n",
    "* $V^T$ gives information about how much each movie is related to latent features.\n",
    "\n",
    "You found that you could iterate to find the latent features in each of these matrices using gradient descent. You wrote a function to implement gradient descent to find the values within these two matrices.\n",
    "\n",
    "Using this method, you were able to make a prediction for any user-movie pair in your dataset. You also could use it to test how well your predictions worked on a train-test split of the data. However, this method fell short with new users or movies.\n",
    "\n",
    "### The Cold Start Problem\n",
    "Collaborative filtering using FunkSVD still wasn't helpful for new users and new movies. In order to recommend these items, you implemented content-based and ranked-based recommendations along with your FunkSVD implementation.\n",
    "\n",
    "> There are so many ways to make recommendations, and this course provides you a very strong mind and skill set to tackle building your own recommendation systems in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Key Term | Definition |\n",
    "|----------|------------|\n",
    "| Classification | Machine learning algorithm used to identify the category of new observations on the basis of training data.|\n",
    "|Cold Start Problem | The problem that new users and new items to a platform don't have any ratings. Because these users and items don't have any ratings, it is impossible to use collaborative filtering methods to make recommendations. |\n",
    "| Fit | A method used to train the model data. |\n",
    "| FSVD | Variation of SVD that is especially useful for matrices with lots of missing values. |\n",
    "| Latent Factors | A feature that isn't actually observed in the data, but can be inferred based on the relationships that occur. |\n",
    "| Nan | Missing data from a dataset |\n",
    "| Offline testing | Testing out your recommendations in a training-testing environment prior to deploying them |\n",
    "| Online testing | Deploying your experiment live and just watch your metrics carefully. |\n",
    "| Linear Regression | Machine learning algorithm that results with a calculated continuous numerical value based on the formula Y = mX+B, given any value for X.|\n",
    "| Sigma matrix\t| Helps give you an idea of how many latent factors we might want to keep.|\n",
    "| SVD | Singular Value Decomposition. An approach to recommendations that uses matrix factorization. This technique falls short for many recommendation problems. |\n",
    "| User group feedback | Validating your recommendations by having user groups give feedback on items you would recommend for them. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c55a5ca6a4c1f1ef62c1036c8fbd5c92f4c8bedcd7e16f2fbeae2f8c4a338b15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
